{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "pd.set_option('max_colwidth', 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jend/fp/RealRelevantReviews\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.7G\n",
      "drwxrwxr-x  4 jend jend 4.0K Jul  3 02:23 .\n",
      "drwxr-xr-x 21 jend jend 4.0K Jul  3 02:22 ..\n",
      "drwxrwxr-x  2 jend jend 4.0K Jul  3 02:01 .ipynb_checkpoints\n",
      "drwxrwxr-x  4 jend jend 4.0K Jul  3 02:36 RealRelevantReviews\n",
      "-rw-rw-r--  1 jend jend 9.3M Jul  2 02:40 rowling.json\n",
      "-rw-rw-r--  1 jend jend  47M Jun 22 04:41 small_train.json\n",
      "-rw-rw-r--  1 jend jend 467K Jun 22 04:37 toy_train.json\n",
      "-rw-rw-r--  1 jend jend 4.6G Jul  3 02:23 train.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"reviewerID\": \"A2S166WSCFIFP5\", \"asin\": \"000100039X\", \"reviewerName\": \"adead_poet@hotmail.com \\\"adead_poet@hotmail.com\\\"\", \"helpful\": [0, 2], \"reviewText\": \"This is one my must have books. It is a masterpiece of spirituality. I'll be the first to admit, its literary quality isn't much. It is rather simplistically written, but the message behind it is so powerful that you have to read it. It will take you to enlightenment.\", \"overall\": 5.0, \"summary\": \"close to god\", \"unixReviewTime\": 1071100800, \"reviewTime\": \"12 11, 2003\"}\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the data\n",
    "reader = pd.read_json(dataset,lines=True,chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_review_date = datetime.fromisoformat('2014-07-23 00:00:00')\n",
    "d = date(2014, 7, 23)\n",
    "t = time(0,0)\n",
    "last_review_date = datetime.combine(d, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunk_helpful_votes = 0\n",
    "all_chunk_unhelpful_votes = 0\n",
    "for chunk in reader:\n",
    "    chunk.reviewTime = pd.to_datetime(chunk.reviewTime,infer_datetime_format=True)\n",
    "    chunk['review_age_days'] = chunk.apply(lambda x: (last_review_date - x.reviewTime).days,axis = 1)\n",
    "\n",
    "    chunk['helpful_votes'] = chunk.apply(lambda x: x.helpful[0],axis=1)\n",
    "    all_chunk_helpful_votes += chunk.helpful_votes.sum()\n",
    "    chunk['unhelpful_votes'] = chunk.apply(lambda x: x.helpful[1],axis=1)\n",
    "    all_chunk_unhelpful_votes += chunk.unhelpful_votes.sum()\n",
    "    \n",
    "    # TODO\n",
    "    #     chunk = chunk[chunk.review_age_days != 0] # somehow we need to drop these to calculate HVAR\n",
    "    #     chunk['HVAR'] = chunk.apply(lambda x: 365*x.helpful_votes/x.review_age_days,axis = 1)\n",
    "    # can above be ifelse, setting to zero if review age is 0days?\n",
    "    \n",
    "    # TODO\n",
    "    # Group by book asin\n",
    "    # Filter training set to books with >= MinRvws count of reviews\n",
    "    # Also filter to books that have variance in their helpful review counts, maybe use std for aggregation\n",
    "    # then how to combine the (much smaller) dataset into one (hopefully loadable) dataset\n",
    "    \n",
    "#total_votes = all_chunk_helpful_votes + all_chunk_unhelpful_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258571 total votes were recorded on the training set reviews; 42.62% were votes indicating helpfulness\n"
     ]
    }
   ],
   "source": [
    "print(\"{} total votes were recorded on the training set reviews; {:.2%} were votes indicating helpfulness\".\\\n",
    "      format(total_votes,all_chunk_helpful_votes/total_votes))\n",
    "# 50258571 total votes were recorded on the training set reviews; 42.62% were votes indicating helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More TODOs:\n",
    "# See TODO section in reader above\n",
    "# How do we incorporate the \"subject\" of the review? Segment/sentence 1?\n",
    "# Will we need to embed the individual book asin IDs in order to train our model? \n",
    "# Do we have enough data to do that?\n",
    "# What would it take to scrape genre data from Amazon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "#Predicting Amazon Book Review Helpfulness using BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSzBwDdGV4bc"
   },
   "source": [
    "This notebook is based on the one Joachim shared in class (which predicted movie review sentiment)\n",
    "https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=p9gEt5SmM6i6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "46f3049d-a458-4db4-e407-1a2343ad69d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    " !pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gWMtEtarfcD"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "LIm6855n-ntE",
    "outputId": "5d126f30-e9eb-4ec0-9c29-c64949e8d7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/32/bf/a105dffbbebd60662bf95c550bb74e4236ca28e77c06735cf6ec11577d0e/gcsfs-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.5)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.0.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth>=1.2->gcsfs) (0.4.5)\n",
      "Installing collected packages: gcsfs\n",
      "Successfully installed gcsfs-0.3.0\n"
     ]
    }
   ],
   "source": [
    "# This is so I can access my google storage bucket later\n",
    "!pip install gcsfs\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "I store output in a GCP bucket. To do this, one must Oauth, so don't let this cell sit and spin; must follow the link and authorize access via Google\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "e0858d92-8cd7-4b3b-fb38-ab0b37e889be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: gs://w266/20190719-23:07_Berty_output *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = datetime.strftime(datetime.now(),\"%Y%m%d-%H:%m\")+'_Berty_output'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = True #@param {type:\"boolean\"}\n",
    "BUCKET = 'w266' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "US7CbDpYeXza",
    "outputId": "6b29b77f-9cbe-4466-cef7-19c2d37505c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud alpha survey\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 23:24:41.764789 140154546886528 _default.py:280] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    }
   ],
   "source": [
    "# Here's where I pull data from storage bucket (requires auth first time)\n",
    "!gcloud config set project w266-239820\n",
    "my_data = \"gs://w266/labeled_dev_set.csv\"\n",
    "mine = pd.read_csv(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYeOtgGb61NN"
   },
   "outputs": [],
   "source": [
    "# Cleanup reviews without review content\n",
    "#mine.iloc[94073]['reviewText'] # Example has 'nan' as reviewText\n",
    "mine.dropna(subset=['reviewText'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qgg4u2vDjzrz",
    "outputId": "03a3027c-1b94-481a-dcfc-b5df344dca48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32447"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many reviews have exactly 0 helpful votes?\n",
    "sum(mine.helpful_votes == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLPh6urafOCp"
   },
   "outputs": [],
   "source": [
    "# Below I sample to have equal amounts of pos/neg reviews and equal amounts of top-quartile-HVAR vs 0 helpful votes\n",
    "num_per_condition = 1000\n",
    "neg_helpful = mine[(mine.overall == 1) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].sample(num_per_condition)\n",
    "neg_unhelpful = mine[(mine.overall == 1) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].sample(num_per_condition)\n",
    "pos_unhelpful = mine[(mine.overall == 5) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].sample(num_per_condition)\n",
    "pos_helpful = mine[(mine.overall == 5) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].sample(num_per_condition)\n",
    "# \"reviewText\" has the review content\n",
    "# \"most_helpful\" has the label of 0 or 1\n",
    "# \"overall\" has the star-rating {1,2,3,4,5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1EPYf0HFijt"
   },
   "outputs": [],
   "source": [
    "# Experiment with prepending stars to the reviews, as a way to pass overall rating to our classifier\n",
    "# neg_helpful['prepReviewText'] = neg_helpful.apply(lambda x: '* ' + x.reviewText,axis = 1)\n",
    "# neg_unhelpful['prepReviewText'] = neg_unhelpful.apply(lambda x: '* ' + x.reviewText,axis = 1)\n",
    "# pos_unhelpful['prepReviewText'] = pos_unhelpful.apply(lambda x: '***** ' + x.reviewText,axis = 1)\n",
    "# pos_helpful['prepReviewText'] = pos_helpful.apply(lambda x: '***** ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seoSOYdEQIg4"
   },
   "outputs": [],
   "source": [
    "# Experiment with prepending TEXT representation of starts to the reviews, \n",
    "# as a way to pass overall rating to our classifier\n",
    "# because haven't figured out how to send categorical data AROUND the transformer yet\n",
    "neg_helpful['prepReviewText'] = neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "neg_unhelpful['prepReviewText'] = neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "pos_unhelpful['prepReviewText'] = pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "pos_helpful['prepReviewText'] = pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YK8PY-Aeftqa",
    "outputId": "a2161966-955b-4244-fbd0-27acaf92e1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset is now 4000 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "stratdf = neg_helpful.append(neg_unhelpful, ignore_index=True)\n",
    "stratdf = stratdf.append(pos_unhelpful, ignore_index=True)\n",
    "stratdf = stratdf.append(pos_helpful, ignore_index=True)\n",
    "print(f\"Our dataset is now {stratdf.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRgAkcNwxbtf"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(stratdf,random_state=42)[['prepReviewText','overall','most_helpful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "wkV9hrkJ3Fd6",
    "outputId": "18bc7036-a9a5-42cc-ed3f-068635896e91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>most_helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>WORST How does Mr. Fisher suggest that you inv...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3491</th>\n",
       "      <td>BEST I come from a technical background and ha...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>WORST I have to say that I am 37 and I bought ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>BEST This is the 2nd book in the series.  I lo...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>BEST Perfect! I honestly did nothing for read ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prepReviewText  overall  most_helpful\n",
       "555   WORST How does Mr. Fisher suggest that you inv...        1             1\n",
       "3491  BEST I come from a technical background and ha...        5             1\n",
       "527   WORST I have to say that I am 37 and I bought ...        1             1\n",
       "3925  BEST This is the 2nd book in the series.  I lo...        5             1\n",
       "2989  BEST Perfect! I honestly did nothing for read ...        5             0"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDfKV6d0RGj5"
   },
   "outputs": [],
   "source": [
    "# This version is ONLY the original reviewText being fed in\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.reviewText,df.most_helpful, test_size=0.2, \\\n",
    "#                                    random_state=42,stratify=df.most_helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WabbdiryclFP"
   },
   "outputs": [],
   "source": [
    "# TODO keep the numerical/categorical rating with the reviewText, once able to input rating on a wide path AROUND nn\n",
    "# X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(data=dict(reviewText=df.reviewText, \\\n",
    "#                                    rating=df.overall)),df.most_helpful, test_size=0.2, \\\n",
    "#                                    random_state=42,stratify=df.most_helpful)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCJ2JkgVH9JV"
   },
   "outputs": [],
   "source": [
    "# Try inputting the rating as 1-5 stars prepended to the review text\n",
    "# If that doesn't work, then bias the thing with 1=Worst, 2=Bad, 3=OK, 4=Good, 5=Best\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.prepReviewText,df.most_helpful, test_size=0.2, \\\n",
    "                                   random_state=42,stratify=df.most_helpful)\n",
    "# Ideally I would like to stratify such that train and test have stratified samples across\n",
    "# BOTH the most_helpful values AND the overall rating, but I keep getting errors when I try to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "5wAjLCMUkqLr",
    "outputId": "483abb51-21c4-4071-ea9c-ec1df4c7d5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 3200 rows and 2 columns.\n",
      "Test has 800 rows and 2 columns.\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([X_train,y_train], axis=1)\n",
    "test = pd.concat([X_test,y_test], axis=1)\n",
    "\n",
    "print(f\"Train has {train.shape[0]} rows and {train.shape[1]} columns.\")\n",
    "print(f\"Test has {test.shape[0]} rows and {test.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Ii8rHyPY3yUr",
    "outputId": "39e2d5ea-f578-4163-fbb9-ff75042821e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>WORST I know the vampire genre has become quit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>BEST After a glowing review full of praise in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>BEST Marquez takes you into a magical tour thr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>WORST I've been reading Horner's book, and I'm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>WORST I believe this book almost perfectly cap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prepReviewText  most_helpful\n",
       "399   WORST I know the vampire genre has become quit...             1\n",
       "3036  BEST After a glowing review full of praise in ...             1\n",
       "2596  BEST Marquez takes you into a magical tour thr...             0\n",
       "77    WORST I've been reading Horner's book, and I'm...             1\n",
       "326   WORST I believe this book almost perfectly cap...             1"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "# DATA_COLUMN = 'reviewText'\n",
    "DATA_COLUMN = 'prepReviewText'\n",
    "LABEL_COLUMN = 'most_helpful'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "8252df03-0e71-4f90-d22a-cbc203663d45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 23:32:00.811939 140154546886528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LL5W8gEGRTAf"
   },
   "outputs": [],
   "source": [
    "# 512 is appaarently upper limit of algo\n",
    "# JenD's current environment maxes out at 13Gig memory\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  # TODO pass in the rating along a separate wide path\n",
    "  # See https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter\n",
    "#   input_rating = keras.layers.Input(shape=[1], name=\"wide_input\")\n",
    "  \n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Using \"pooled_output\" for classification task of each truncated review.\n",
    "  deep_layer = bert_outputs[\"pooled_output\"]\n",
    "  deep_size = deep_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for helpfulness data\n",
    "  # TODO \"+1\" needed on the deep_size, to weight the rating value arriving from the side\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, deep_size ],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # TODO Prepend the rating of the review\n",
    "    # concatting_layer = keras.layers.Concatenate()([input_rating, deep_layer])\n",
    "    \n",
    "    # TODO pass concatting layer to dropout\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(deep_layer, keep_prob=.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_WebpS1X97v"
   },
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "0be692f8-66e1-4432-d7ae-a7a054dbf1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0719 23:40:27.818220 140154546886528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0719 23:41:14.365194 140154546886528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:10:38.858289\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "a78a32df-b22e-46e3-87cf-7fbe8f9d4e82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.68375,\n",
       " 'eval_accuracy': 0.68375,\n",
       " 'f1_score': 0.6666666,\n",
       " 'false_negatives': 158.0,\n",
       " 'false_positives': 95.0,\n",
       " 'global_step': 300,\n",
       " 'loss': 0.720908,\n",
       " 'precision': 0.7181009,\n",
       " 'recall': 0.605,\n",
       " 'true_negatives': 305.0,\n",
       " 'true_positives': 242.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUDBcFUKOfzX"
   },
   "source": [
    "Above prepended with * or ***** on train/test data built from 4 conditions across (1*,5*) and (most_helpful==1,0) with training on 3200 rows and testing on 800 rows (stratified on helpful/not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "  labels = [\"Unhelpful\", \"Helpful\"]\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "iyAOOru-T0aL",
    "outputId": "926d49a7-0031-4c1f-e68d-d6d701840306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sample tokenizes to 193 tokens.\n",
      "Beginning with:  \n",
      " \t ['best', 'as', 'someone', 'who', 'is']\n",
      "And ending with:  \n",
      " \t ['leaves', 'my', 'kitchen', 'counter', '.']\n"
     ]
    }
   ],
   "source": [
    "# Here I have a few \"canary in the coal mine\" examples\n",
    "# If the model can't get these right, I put very little faith in it\n",
    "\n",
    "# This is 1* without sharing why\n",
    "vacuous_negative = \"WORST I like to read many different books from many points of view.  This book was a complete waste of time that I will never get back. Save yourself the trouble and money.\"\n",
    "\n",
    "# This is 5* but worthless\n",
    "vacuous_positive = \"BEST My husband has really loved this series and I have heard the same comments from many others who have also read this series.\"\n",
    "\n",
    "# This looks like someone who didn't read the book was paid to write a long 5* review\n",
    "# it sounds like a completely generic description of any cookbook (\"it provides recipes to prepare foods...\")\n",
    "# Perhaps mentions of many concepts (ingredients, gourmet, etc.) can fool some people and also an algorithm\n",
    "# Helpful votes: 71\n",
    "# Annual HVAR: 5\n",
    "# For this book, the top quartile was HVAR of 1.7\n",
    "# Surely adding GENRE would knock this one down\n",
    "vacuous_cookbook = \"BEST As someone who is learning to cook only late in her life, I was apprehensive and embarrassed about asking simple basic questions of friends and family.  Perceiving this, my parents gave me this cookbook, and voila!  -- I can cook!With step-by-step instructions on everything from  cookware, ingredients, buying, preapring, cooking, and serving, there's  nothing this book can't handle.  It provides recipes to prepare foods in  the simplest ways, all the way up to complex gourmet dishes.  And it covers  every imaginable food -- if it isn't in here, I can't imagine where you'd  find it.The language is straightforward and encouraging, with  appropriate editorializing on the author's preferences, and the layout is  clean and easy to read.  I can't say enough good things about this cookbook  -- it never leaves my kitchen counter.\"\n",
    "\n",
    "# Reviewer hasn't read it yet\n",
    "vacuous_not_read = \"BEST Just downloaded this series.  Looking forward to getting to read it, once i get past some of the other books on my reading list.  I just love beig able to carryall these books without having to carry them individually!\"\n",
    "\n",
    "# This excerpt of a review got 5* and JenD finds it helpful \n",
    "meaty_positive = \"BEST After \\\"Riding Lessons\\\", which I loved - and \\\"Flying Changes\\\", which was a huge disappointment to me, I was not sure what I would find in \\\"Water for Elephants\\\".  Wow - what a great read!  Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara.  This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Reading \\\"Water for Elephants\\\" is time well-spent.  I'm happy to know Sara is working on a fourth book. I'll be first in line to buy it.\"\n",
    "#Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara. This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Time well spent!\"\n",
    "\n",
    "# Another excerpt from a 5* with many helpful reviews\n",
    "meaty_positive2 = \"BEST This had the flavor of The Great Gatsby, the well to do characters spend their days going from luncheon to evening parties and everyone is concerned about who's who. That is, on the surface it has that flavor. Beneath is a gripping story more about Trudy and Will than about the piano teacher, Claire and the desperate desire to survive.The characters in this book are well developed. There is a lot going on beneath the surface that the author lets you discern.Life in Hong Kong during the 40's is a lark and all about the parties you go to, until the Japanese occupation.  Will is interned along with many of the other socialites.  Life becomes getting food, keeping warm, keeping from being infested, keeping from being singled out for abuse.\"\n",
    "\n",
    "# This excerpt is from a negative 1* review and includes arguments (>300 helpful votes) \n",
    "meaty_negative = \"WORST Colin Campbell is so intent on promoting a vegan data that he misrepresents the data in the real China Study and cherry picks anti-animal food data. For instance, he rightly cites the link between milk and autoimmune disease but fails to mention that gluten, from wheat and related grains, is at least as important a cause. He writes of the association between casein, a milk protein, with cancer, but fails to mention that whey and butterfat are protective against cancer, and in milk you get all of them. He makes completely false statements like folate not being in meat when organ meats are much higher in folate than any plant source according to the USDA. He assumes nutrient consistency with the US without actually measuring it, despite the fact that soil nutrients and species differences have a huge effect on nutrition.\"\n",
    "\n",
    "# This is a scholar reviewing another scholar's 5* work\n",
    "# Helpful votes: 788 and annual HVAR: 74\n",
    "meaty_scholar = \"BEST Noted historian of the early church Elaine Pagels has produced a clear, cogent, and very effective introduction to the subject of Gnosticism, a different form of Christianity that was declared heretical and virtually stamped out by the orthodox church by the start of the second century after Christ.  Most of what we knew of the Gnostic belief system came from the religious authors who worked so hard to destroy the movement, but that changed drastically with the still relatively recent discovery of a number of lost Gnostic writings near Nag Hammadi in Upper Egypt.  Unlike the Dead Sea Scrolls, this momentous discovery of ancient papyri has received little attention, and I must admit I went into this book knowing virtually nothing about Gnosticism.  As an historian by training and a Christian, the information in these &quot;heretical&quot; texts intrigue me, and I believe that Christians should challenge their faith by examining material that does not fall in line\"\n",
    "\n",
    "# Make the list of canaries\n",
    "pred_sentences = [vacuous_negative,\\\n",
    "                  vacuous_positive, \\\n",
    "                  vacuous_cookbook, \\\n",
    "                  vacuous_not_read, \\\n",
    "                  meaty_negative, \\\n",
    "                  meaty_positive, \\\n",
    "                  meaty_positive2, \\\n",
    "                  meaty_scholar]\n",
    "\n",
    "# See how any sentence looks in tokens by uncommenting below\n",
    "to_tokens = vacuous_cookbook\n",
    "tokenized = tokenizer.tokenize(to_tokens)\n",
    "print(f\"That sample tokenizes to {len(tokenized)} tokens.\")\n",
    "print(\"Beginning with: \",\"\\n\",'\\t',tokenized[0:5])\n",
    "print(\"And ending with: \",\"\\n\",'\\t',tokenized[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "ad7727ac-2069-4390-f30c-363bf190c10c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WORST I like to read many different books from many points of view.  This book was a complete waste of time that I will never get back. Save yourself the trouble and money.',\n",
       "  array([-0.05290221, -2.9656446 ], dtype=float32),\n",
       "  'Unhelpful'),\n",
       " ('BEST My husband has really loved this series and I have heard the same comments from many others who have also read this series.',\n",
       "  array([-0.06328475, -2.7915869 ], dtype=float32),\n",
       "  'Unhelpful'),\n",
       " (\"BEST As someone who is learning to cook only late in her life, I was apprehensive and embarrassed about asking simple basic questions of friends and family.  Perceiving this, my parents gave me this cookbook, and voila!  -- I can cook!With step-by-step instructions on everything from  cookware, ingredients, buying, preapring, cooking, and serving, there's  nothing this book can't handle.  It provides recipes to prepare foods in  the simplest ways, all the way up to complex gourmet dishes.  And it covers  every imaginable food -- if it isn't in here, I can't imagine where you'd  find it.The language is straightforward and encouraging, with  appropriate editorializing on the author's preferences, and the layout is  clean and easy to read.  I can't say enough good things about this cookbook  -- it never leaves my kitchen counter.\",\n",
       "  array([-2.7105293 , -0.06881603], dtype=float32),\n",
       "  'Helpful'),\n",
       " ('BEST Just downloaded this series.  Looking forward to getting to read it, once i get past some of the other books on my reading list.  I just love beig able to carryall these books without having to carry them individually!',\n",
       "  array([-0.05398706, -2.9458833 ], dtype=float32),\n",
       "  'Unhelpful'),\n",
       " ('WORST Colin Campbell is so intent on promoting a vegan data that he misrepresents the data in the real China Study and cherry picks anti-animal food data. For instance, he rightly cites the link between milk and autoimmune disease but fails to mention that gluten, from wheat and related grains, is at least as important a cause. He writes of the association between casein, a milk protein, with cancer, but fails to mention that whey and butterfat are protective against cancer, and in milk you get all of them. He makes completely false statements like folate not being in meat when organ meats are much higher in folate than any plant source according to the USDA. He assumes nutrient consistency with the US without actually measuring it, despite the fact that soil nutrients and species differences have a huge effect on nutrition.',\n",
       "  array([-2.9574134 , -0.05335132], dtype=float32),\n",
       "  'Helpful'),\n",
       " ('BEST After \"Riding Lessons\", which I loved - and \"Flying Changes\", which was a huge disappointment to me, I was not sure what I would find in \"Water for Elephants\".  Wow - what a great read!  Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara.  This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Reading \"Water for Elephants\" is time well-spent.  I\\'m happy to know Sara is working on a fourth book. I\\'ll be first in line to buy it.',\n",
       "  array([-2.1425905 , -0.12482701], dtype=float32),\n",
       "  'Helpful'),\n",
       " (\"BEST This had the flavor of The Great Gatsby, the well to do characters spend their days going from luncheon to evening parties and everyone is concerned about who's who. That is, on the surface it has that flavor. Beneath is a gripping story more about Trudy and Will than about the piano teacher, Claire and the desperate desire to survive.The characters in this book are well developed. There is a lot going on beneath the surface that the author lets you discern.Life in Hong Kong during the 40's is a lark and all about the parties you go to, until the Japanese occupation.  Will is interned along with many of the other socialites.  Life becomes getting food, keeping warm, keeping from being infested, keeping from being singled out for abuse.\",\n",
       "  array([-2.1048834 , -0.12994914], dtype=float32),\n",
       "  'Helpful'),\n",
       " ('BEST Noted historian of the early church Elaine Pagels has produced a clear, cogent, and very effective introduction to the subject of Gnosticism, a different form of Christianity that was declared heretical and virtually stamped out by the orthodox church by the start of the second century after Christ.  Most of what we knew of the Gnostic belief system came from the religious authors who worked so hard to destroy the movement, but that changed drastically with the still relatively recent discovery of a number of lost Gnostic writings near Nag Hammadi in Upper Egypt.  Unlike the Dead Sea Scrolls, this momentous discovery of ancient papyri has received little attention, and I must admit I went into this book knowing virtually nothing about Gnosticism.  As an historian by training and a Christian, the information in these &quot;heretical&quot; texts intrigue me, and I believe that Christians should challenge their faith by examining material that does not fall in line',\n",
       "  array([-2.9720075 , -0.05255775], dtype=float32),\n",
       "  'Helpful')]"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = getPrediction(pred_sentences)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq9GdFp8-Wkz"
   },
   "source": [
    "JenD is actually pretty happy with the above results, from a canary-in-the-coalmine point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Mu1Jcnkzs5H"
   },
   "outputs": [],
   "source": [
    "# Now combine the test data and the test predictions, for some error analysis\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bdqu5CGFu2J1"
   },
   "outputs": [],
   "source": [
    "test_preds = getPrediction(list(test.prepReviewText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgzVcz0Gvmt0"
   },
   "outputs": [],
   "source": [
    "testy_preds = pd.DataFrame(test_preds, columns=['text','probas','judgment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2gX28GMzj4P"
   },
   "outputs": [],
   "source": [
    "beforeAndAfter = pd.merge(left=test, right=testy_preds,\n",
    "                                  left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "12VwMM-a0Q_9",
    "outputId": "467793b1-db9b-4827-9fbf-8ceb61acbe1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>text</th>\n",
       "      <th>probas</th>\n",
       "      <th>judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>399</td>\n",
       "      <td>WORST I zipped through the first book of this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>WORST I zipped through the first book of this ...</td>\n",
       "      <td>[-0.0693467, -2.70311]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3036</td>\n",
       "      <td>BEST I love the Rylie Cruz series and all of R...</td>\n",
       "      <td>1</td>\n",
       "      <td>BEST I love the Rylie Cruz series and all of R...</td>\n",
       "      <td>[-2.6566658, -0.07276628]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2596</td>\n",
       "      <td>BEST I am very happy with this book.  I haven'...</td>\n",
       "      <td>0</td>\n",
       "      <td>BEST I am very happy with this book.  I haven'...</td>\n",
       "      <td>[-0.043234207, -3.1626618]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>WORST Let me be honest here, although this cou...</td>\n",
       "      <td>1</td>\n",
       "      <td>WORST Let me be honest here, although this cou...</td>\n",
       "      <td>[-0.78407216, -0.60980487]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>326</td>\n",
       "      <td>WORST I absolutely hated this book.  The heroi...</td>\n",
       "      <td>1</td>\n",
       "      <td>WORST I absolutely hated this book.  The heroi...</td>\n",
       "      <td>[-2.70472, -0.06923114]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...   judgment\n",
       "0    399  ...  Unhelpful\n",
       "1   3036  ...    Helpful\n",
       "2   2596  ...  Unhelpful\n",
       "3     77  ...    Helpful\n",
       "4    326  ...    Helpful\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beforeAndAfter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-5v64bb06il"
   },
   "outputs": [],
   "source": [
    "beforeAndAfter.drop(['text'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NTXi3S1wcPb"
   },
   "outputs": [],
   "source": [
    "fndf = beforeAndAfter[(beforeAndAfter.most_helpful==1) & (beforeAndAfter.judgment=='Unhelpful')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bmitB4bx1ZjY",
    "outputId": "75175b13-7282-4641-aaa0-70011fe49fd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 5)"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fndf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TgyOBj30pWy"
   },
   "outputs": [],
   "source": [
    "fpdf = beforeAndAfter[(beforeAndAfter.most_helpful==0) & (beforeAndAfter.judgment=='Helpful')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7Yd3aQA2FDU"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "DZIqYUEC6q_k",
    "outputId": "1898d10c-f9a0-4bb3-ac5b-3f1f37c190bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>probas</th>\n",
       "      <th>judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2517</td>\n",
       "      <td>BEST If you could prove or disprove faith-based knowledge (religious beliefs), faith would not be necessary. Nevertheless, rabid theists engage in attempts to prove, and rabid atheists engage in attempts to disprove. And predictably, they come up as empty-handed as the person referred to in the old adage, \"You can't squeeze blood out of a turnip.\" It's impossible, but since hope springs eternal in matters of faith, and too, there is a market for controversy that yields big bucks in book royalties, rabid theists and atheists remain engaged.Unlike others in the current crop of rabid atheists, Harris does not play the game of attempting to disprove faith-based knowledge--at least not in his book, The End of Faith: Religion, Terror, and the Future of Reason. In the philosophical tradition of logical positivism, he asserts instead that without the objective criterion of evidence to support faith-based knowledge, it is nothing more than fairy tale. He goes on to argue that without that o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-2.821497, -0.061361484]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2811</td>\n",
       "      <td>BEST I read \"Princess in Waiting\" in 2 sittings.I loved it because it is so wonderfully good. And coming from me, that should mean something, because I am a teen, but I don't really like teen books too much, with this series as one of my few exceptions. This fourth book lived up to the other three, it was addictive. This series does keep getting better, with each book entrancing you, and makes you unable to put it down, even as you try, as \"Princess In Waiting\" did. One critic said it is like \" reading a note from your best friend\", and that is so true.You get caught up in the story and the people, that is one of the great qualities about it.You think it is your life, and that you know them. This is the type of book/series that you wait for months for the next one to be released. I did, and I wasn't disappointed. Read it, you won't be either.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-2.3271813, -0.10266453]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2159</td>\n",
       "      <td>BEST I'm building a library of books on fashion, and this one is a keeper. Very nice photos and good descriptions of the dresses.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.9282413, -0.5029372]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2129</td>\n",
       "      <td>BEST \"The Capitol Game\" by Brian Haig starts off with an American soldier and his group getting killed in a roadside explosion in Iraq. While wondering how the incident might have been prevented, Haig moves us to present day. An ambitious Wall Street businessman, Jack Wiley, thinks he's found the next billion dollar business--a company on the verge of bankruptcy whose founder has discovered a special type of polymer that makes military vehicles in addition to other transportation, virtually explosion-proof.Wiley tries to sell the idea to the wealthy Capitol Group--a company whose whole business is based on taking over companies and making them profitable. However, Wiley won't tell them any more details about the venture without being guaranteed a solid percentage of future income from the polymer and being charge of the takeover himself. The Capitol Group grows suspicious of Jack, and starts concocting ways of getting its greedy claws on the polymer while moving Jack to the side. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.91135, -0.16002858]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2163</td>\n",
       "      <td>BEST Several years ago I penned an essay I \"Ten Steps to Eloquence.\"  In my mind, the final step was the most important:  the delivery.Although he may never have read it, its message was lost on Jerry Weissman.  A presentation coach with a long list of corporate clients uses his third book to present a seven-step plan for crafting content into a compelling story. Weissman teaches how to overcome public speaking jitters, present with force and conviction and to emotionally connect with any audience.  Beyond your words, the author demonstrates how to communicate with your audience using your body language.Readers of the book have access to a website that provides case studies of power presenters--from Martin Luther King, Jr. to John F. Kennedy, from Ronald Reagan to Barack Obama.If you need to deliver, this is a book you cannot afford to miss.  It is filled with techniques, tools and wisdom gathered during long career communicating with audiences.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-2.768761, -0.06479424]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  ... judgment\n",
       "8    2517  ...  Helpful\n",
       "25   2811  ...  Helpful\n",
       "70   2159  ...  Helpful\n",
       "83   2129  ...  Helpful\n",
       "85   2163  ...  Helpful\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some false positives that were 5* (which is what prepended with BEST means)\n",
    "fpdf[fpdf.prepReviewText.str.contains('BEST')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "HHZizSa91mcc",
    "outputId": "caa280ba-c64c-4ebb-af3f-9143236b7150"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>probas</th>\n",
       "      <th>judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1726</td>\n",
       "      <td>WORST I love Anne Lamott, and I love that her son was a willing participant in this memoir. This truly is a labor of God.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.0120432, -0.45173246]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1318</td>\n",
       "      <td>WORST I love Amish fiction and have read every complete series by all other authors and loved every book.  I held off on these books only because the author is male and I wasn't sure he could write with the passion that the female authors do. I was right.  I suffered through 10 chapters of this book before putting it down for good.  Characters are uninteresting and the details are different.  I cannot put my finger on exactly how his details are different, but they are uninteresting.  I especially did not like the one married female Amish character who was uncharacteristically obsessed with money.  Too much worry, and not enough faith in God as I expect to read in Amish fiction. I do not normally give negative reviews.  This is only the second book I have ever stopped reading in many years of reading Christian fiction.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.0489589, -0.43124798]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1028</td>\n",
       "      <td>WORST Wow, after reading and enjoying the author's first book, Joshua, I fully expected to enjoy his second book as well, Traveler. Not so! The tone of Traveler was so different from Joshua that it truly felt as if a different author had penned it. Joshua was a great character study. The characters and relationships developed as the book proceeded, and the love that was so evident between \"the man\" and Joshua carried the book even when their survival efforts were dark and discouraging. But Traveler ... nothing but disappointing. No character development, no one in the story to care about. In my opinion this book was a waste of time. What a shame, after such a valiant first novel!</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.7100235, -0.19950213]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1299</td>\n",
       "      <td>WORST This is the only book so far that I almost couldn't finish.  It was awful! Not bad enough that she not only shoved ('crammed' might be a better word) her religious....techniques (not just faith) on someone else, but waited until he fell in love with her to do it.  This was not only long and unendurable, but completely unbelievable as a story.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.84565264, -0.5608514]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1441</td>\n",
       "      <td>WORST I was suckered into this after a promising \"sample\" on Kindle.  Completely boring and poorly written.  I didn't finish this.  It was so boring that I can't even muster up the interest to really analyze why I didn't like it.  I just felt I had to do my civic duty and add my *1 star* to warn off readers.  Try ANYTHING by Liz Carlyle, Julia Quinn, Suzanne Enoch or Lisa Kleypas as a better option.  Even some of the second tier novelists in this genre are a safer bet.</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.799672, -0.59688544]</td>\n",
       "      <td>Helpful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  ... judgment\n",
       "11   1726  ...  Helpful\n",
       "12   1318  ...  Helpful\n",
       "13   1028  ...  Helpful\n",
       "28   1299  ...  Helpful\n",
       "47   1441  ...  Helpful\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some false positives that were 1*\n",
    "fpdf[fpdf.prepReviewText.str.contains('WORST')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "r5y0i6Hw1hCM",
    "outputId": "961857fa-c9ea-4828-cb2e-1c6edff37e81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>probas</th>\n",
       "      <th>judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3155</td>\n",
       "      <td>BEST I usually never write reviews on a book in fact this is my 1st one. C.J.'s VV Inn series is awesome. I loved every page. From the hot sex that Rafe and Dria have to the werewolves getting shot and not knowing who was going to get killed. She goes into such detail on her characters and really brings them to life. I have read many vampire books but not 1 as unusual as this series and it was a fantastic to read. I am so excited for book 4.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.12263182, -2.1592581]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>3187</td>\n",
       "      <td>BEST Even though my profession finds me surrounded by books all day I am a tough sell when it comes to sitting down and reading just for fun, but Changing Shoes is an excellent enjoyable read. I laughed. I cried and I thoroughly enjoyed reading it from cover to cover. You do not have to have been a Guiding Light or soap fan to enjoy piece. It appeals to women of all ages. It makes you think about what life really is all about. I feel truly blessed to have had the opportunity to get to know Tina!</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.13903199, -2.0417619]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3668</td>\n",
       "      <td>BEST I love avery and sean they had me laughing and very frustrated. I love hm wards books have read alot of them and loved them. However it frustrates me when I have to  wait for the next one!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.034964226, -3.3708603]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>3911</td>\n",
       "      <td>BEST I moved up to a 4g smart phone and this book saved me. I used it everyday for two weeks, now I just grab it to fiqure out a new section. Really enhances how to use a smart phone.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.062175777, -2.8087165]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>3370</td>\n",
       "      <td>BEST The first time I read this book was at the age of 13. My mother had an old, worn, 1950s copy of the book on our bookshelf, and one day I just decided to give it a try. I think the first 50 pages were a bit of a mystery to me. I remember struggling with the language at the beginning, and having to look up several words in the dictionary. After 100 pages, I had finally \"learnt the language,\" and managed to at last translate it into my native tongue. I enjoyed it, and over the years I returned to it again and again. Now, at the age of 35, I practically have numerous passages memorized, and reading the book is no longer a culture shock. Every word of this novel is an old friend, and I wouldn't remove a single one from the text. This is the book I turn to when I'm in the mood to read, but not in the mood to read something new.Every time I read it, I am struck by its relevance. I don't mean the morals or the need to marry well in order to win approval from society (although I think ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.21256766, -1.6528965]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  ...   judgment\n",
       "96    3155  ...  Unhelpful\n",
       "333   3187  ...  Unhelpful\n",
       "195   3668  ...  Unhelpful\n",
       "476   3911  ...  Unhelpful\n",
       "103   3370  ...  Unhelpful\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some false negatives that were 5* \n",
    "fndf[fndf.prepReviewText.str.contains('BEST')].sample(5)\n",
    "# Hmmm...JenD thinks maybe the algorithm got these right. Yuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "DtmeI2IW4aja",
    "outputId": "3d5293cf-11a1-4e48-90c0-79479029e7f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>probas</th>\n",
       "      <th>judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>754</td>\n",
       "      <td>WORST This is a short story with unconvincing characters and no plot that got stretched out to novella length. I was bored to tears reading it.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.11983059, -2.1809936]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>722</td>\n",
       "      <td>WORST I can't believe this is a best-seller. Honestly, I can't believe it ever made it to print. Read Kushiel's Dart instead.It is very badly written, repetetive, poorly edited (if edited at all), boring ... the only thing I can think of, is that this was written by an illiterate for illiterates. It shows badly on our world and our education system that it is a best-seller. I suspect there is a higher literacy rate among the feral cat colony outside than among her readership. Still, she is laughing all the way to the bank, isn't she?There are enough reviews that give details. I just want to cast a surprisingly minority vote for absolutely awful. I admit, I couldn't finish it. I did try, but this is god-awful.I really don't get the appeal of badly-written, soccer-mom porn.I'm going to feed the cats now. The back of the can of cat food is better-written than this.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.49281728, -0.94392633]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>188</td>\n",
       "      <td>WORST The story line and the first few pages caught my attention but after getting into the book a bit more I found myself putting it down and deleting it from my kindle.  The writing lacks.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.05708686, -2.8915882]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>208</td>\n",
       "      <td>WORST What can I say? Never read a Nora Roberts book that I liked. So I'm the dummy for keepin' on tryin'. I keep thinking that there must be something here simply because she sells so many books, but only goes to show you that there are, apparently, a lot of people who think differently than I do. I wish someone could explain the allure of her books to me. Sorry to be so negative, but I seem to be totally missing something.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.18930922, -1.7575356]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>385</td>\n",
       "      <td>WORST The reason most women find this book exciting, is because the psychology used in this book is what attracts women to men and keeps them interested.It does NOT work the other way around, not for the long term at least. Its a completely different set of emotional priciples that keep men hanging in there for the long haul.Playing aloof to the guy who finally sets your heart on fire, is a sure way to put that fire out. Not to mention all the wonderful emotional thrills you would lose out on, which is reason to be in a relationship in the first place.The only reason to get this book is if:1) You are a total doormat.2) You want a guy to be your doormat, in which case you need to find a total wimp. But ask yourself is this the kind of guy who pulls your heartstrings?3) You want to have an open realtionship (seeing other people outside of the realtionship).So be Bitch at your own risk---you may well lose your soul mate when you finally meet him.</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.16153044, -1.9027398]</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  ...   judgment\n",
       "491    754  ...  Unhelpful\n",
       "160    722  ...  Unhelpful\n",
       "80     188  ...  Unhelpful\n",
       "252    208  ...  Unhelpful\n",
       "187    385  ...  Unhelpful\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some false negatives that were 1* \n",
    "fndf[fndf.prepReviewText.str.contains('WORST')].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHHNVkRZ9kf6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Amazon Book Review Helpfulness using BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

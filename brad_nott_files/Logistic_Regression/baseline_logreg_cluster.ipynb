{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Logistic Regression (clustered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brad\\\\Desktop\\\\Keras - GPU\\\\Baseline Models'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset by sampling labeled_training_set_clust.csv\n",
    "\n",
    "#my_data = \"../w266_proj/data/labeled_training_set_clust.csv\"\n",
    "#mine = pd.read_csv(my_data)\n",
    "\n",
    "# Scaled annual hvar using global min-max values\n",
    "#my_data = \"../w266_proj/data/labeled_training_set_clust_v3.csv\"\n",
    "#mine = pd.read_csv(my_data)\n",
    "\n",
    "# Class labels calculated using annual hvar values from test/dev/train\n",
    "# - calculated min, max, mean, standard dev values to facilitate cluster assignment\n",
    "#my_data = \"../w266_proj/data/labeled_training_set_clust_v4.csv\"\n",
    "#mine = pd.read_csv(my_data)\n",
    "\n",
    "# Class labels calculated using annual hvar values from test/dev\n",
    "# - calculated min, max, mean, standard dev values to facilitate cluster assignment\n",
    "#my_data = \"../w266_proj/data/labeled_training_set_clust_v5.csv\"\n",
    "#mine = pd.read_csv(my_data)\n",
    "\n",
    "train = pd.read_csv(\"../w266_proj/data/labeled_training_set_clust_FINAL.csv\")\n",
    "dev = pd.read_csv(\"../w266_proj/data/labeled_dev_set_clust_FINAL.csv\")\n",
    "test = pd.read_csv(\"../w266_proj/data/labeled_test_set_clust_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data has columns to test 3 types of labeling approaches:**\n",
    "1. Jen's original approach (`most_helpful`)\n",
    "2. 2-class cluster assigned labels (`class_2`)\n",
    "3. 2-class cluster assigned labels (`group_z_class`)\n",
    "\n",
    "**Approach 3 resulted in the most-favorable label assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup reviews without review content\n",
    "#mine.iloc[94073]['reviewText'] # Example has 'nan' as reviewText\n",
    "#mine.dropna(subset=['reviewText'],inplace=True)\n",
    "\n",
    "train.dropna(subset=['reviewText'],inplace=True)\n",
    "dev.dropna(subset=['reviewText'],inplace=True)\n",
    "test.dropna(subset=['reviewText'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271443\n",
      "32445\n",
      "32456\n"
     ]
    }
   ],
   "source": [
    "# How many reviews have exactly 0 helpful votes?\n",
    "#sum(mine.helpful_votes == 0)\n",
    "\n",
    "print(sum(train.helpful_votes == 0))\n",
    "print(sum(dev.helpful_votes == 0))\n",
    "print(sum(test.helpful_votes == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2348039\n",
       "1.0     311377\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.group_z_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    281301\n",
       "1.0     32779\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.group_z_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    281029\n",
       "1.0     32573\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.group_z_class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model: Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_helpful: (31348, 21)\n",
      "neg_unhelpful: (13033, 21)\n",
      "pos_unhelpful: (121879, 21)\n",
      "pos_helpful: (158714, 21)\n"
     ]
    }
   ],
   "source": [
    "# How many examples of each group in training set?\n",
    "print('neg_helpful: {}'.format(train[(train.overall == 1) & (train.group_z_class == 1) & (train.helpful_votes != 0)].shape))\n",
    "print('neg_unhelpful: {}'.format(train[(train.overall == 1) & (train.group_z_class == 0) & (train.helpful_votes == 0)].shape))\n",
    "print('pos_unhelpful: {}'.format(train[(train.overall == 5) & (train.group_z_class == 0) & (train.helpful_votes == 0)].shape))\n",
    "print('pos_helpful: {}'.format(train[(train.overall == 5) & (train.group_z_class == 1) & (train.helpful_votes != 0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Training Set\n",
    "# sample to have equal amounts of pos/neg reviews and equal amounts of top-quartile-HVAR vs 0 helpful votes\n",
    "\n",
    "num_per_condition = 13033\n",
    "repl=False\n",
    "\n",
    "train_neg_helpful = train[(train.overall == 1) & (train.group_z_class == 1) & (train.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "train_neg_unhelpful = train[(train.overall == 1) & (train.group_z_class == 0) & (train.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "train_pos_unhelpful = train[(train.overall == 5) & (train.group_z_class == 0) & (train.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "train_pos_helpful = train[(train.overall == 5) & (train.group_z_class == 1) & (train.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313602, 19)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_helpful: (3418, 21)\n",
      "neg_unhelpful: (1504, 21)\n",
      "pos_unhelpful: (14786, 21)\n",
      "pos_helpful: (16719, 21)\n"
     ]
    }
   ],
   "source": [
    "# How many examples of each group in dev set?\n",
    "print('neg_helpful: {}'.format(dev[(dev.overall == 1) & (dev.group_z_class == 1) & (dev.helpful_votes != 0)].shape))\n",
    "print('neg_unhelpful: {}'.format(dev[(dev.overall == 1) & (dev.group_z_class == 0) & (dev.helpful_votes == 0)].shape))\n",
    "print('pos_unhelpful: {}'.format(dev[(dev.overall == 5) & (dev.group_z_class == 0) & (dev.helpful_votes == 0)].shape))\n",
    "print('pos_helpful: {}'.format(dev[(dev.overall == 5) & (dev.group_z_class == 1) & (dev.helpful_votes != 0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Dev Set\n",
    "# 1504 = no oversampling\n",
    "\n",
    "num_per_condition = 1504\n",
    "repl=False\n",
    "\n",
    "dev_neg_helpful = dev[(dev.overall == 1) & (dev.group_z_class == 1) & (dev.helpful_votes != 0)].sample(num_per_condition, replace=repl)\n",
    "dev_neg_unhelpful = dev[(dev.overall == 1) & (dev.group_z_class == 0) & (dev.helpful_votes == 0)].sample(num_per_condition, replace=repl)\n",
    "dev_pos_unhelpful = dev[(dev.overall == 5) & (dev.group_z_class == 0) & (dev.helpful_votes == 0)].sample(num_per_condition, replace=repl)\n",
    "dev_pos_helpful = dev[(dev.overall == 5) & (dev.group_z_class == 1) & (dev.helpful_votes != 0)].sample(num_per_condition, replace=repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_helpful: (3493, 19)\n",
      "neg_unhelpful: (1533, 19)\n",
      "pos_unhelpful: (14569, 19)\n",
      "pos_helpful: (16272, 19)\n"
     ]
    }
   ],
   "source": [
    "# How many examples of each group in test set?\n",
    "print('neg_helpful: {}'.format(test[(test.overall == 1) & (test.group_z_class == 1) & (test.helpful_votes != 0)].shape))\n",
    "print('neg_unhelpful: {}'.format(test[(test.overall == 1) & (test.group_z_class == 0) & (test.helpful_votes == 0)].shape))\n",
    "print('pos_unhelpful: {}'.format(test[(test.overall == 5) & (test.group_z_class == 0) & (test.helpful_votes == 0)].shape))\n",
    "print('pos_helpful: {}'.format(test[(test.overall == 5) & (test.group_z_class == 1) & (test.helpful_votes != 0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Set\n",
    "# 1533 = no oversampling\n",
    "\n",
    "num_per_condition = 1533\n",
    "repl=False\n",
    "\n",
    "test_neg_helpful = test[(test.overall == 1) & (test.group_z_class == 1) & (test.helpful_votes != 0)].sample(num_per_condition, replace=repl)\n",
    "test_neg_unhelpful = test[(test.overall == 1) & (test.group_z_class == 0) & (test.helpful_votes == 0)].sample(num_per_condition, replace=repl)\n",
    "test_pos_unhelpful = test[(test.overall == 5) & (test.group_z_class == 0) & (test.helpful_votes == 0)].sample(num_per_condition, replace=repl)\n",
    "test_pos_helpful = test[(test.overall == 5) & (test.group_z_class == 1) & (test.helpful_votes != 0)].sample(num_per_condition, replace=repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend training set\n",
    "\n",
    "train_neg_helpful['prepReviewText'] = train_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "train_neg_unhelpful['prepReviewText'] = train_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "train_pos_unhelpful['prepReviewText'] = train_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "train_pos_helpful['prepReviewText'] = train_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend dev set\n",
    "\n",
    "dev_neg_helpful['prepReviewText'] = dev_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "dev_neg_unhelpful['prepReviewText'] = dev_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "dev_pos_unhelpful['prepReviewText'] = dev_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "dev_pos_helpful['prepReviewText'] = dev_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend test set\n",
    "\n",
    "test_neg_helpful['prepReviewText'] = test_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "test_neg_unhelpful['prepReviewText'] = test_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "test_pos_unhelpful['prepReviewText'] = test_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "test_pos_helpful['prepReviewText'] = test_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset is now 52132 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Assemble training set\n",
    "\n",
    "stratdf_train = train_neg_helpful.append(train_neg_unhelpful, ignore_index=True)\n",
    "stratdf_train = stratdf_train.append(train_pos_unhelpful, ignore_index=True)\n",
    "stratdf_train = stratdf_train.append(train_pos_helpful, ignore_index=True)\n",
    "print(f\"Our training dataset is now {stratdf_train.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our development dataset is now 6016 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Assemble dev set\n",
    "\n",
    "stratdf_dev = dev_neg_helpful.append(dev_neg_unhelpful, ignore_index=True)\n",
    "stratdf_dev = stratdf_dev.append(dev_pos_unhelpful, ignore_index=True)\n",
    "stratdf_dev = stratdf_dev.append(dev_pos_helpful, ignore_index=True)\n",
    "print(f\"Our development dataset is now {stratdf_dev.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test dataset is now 6132 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Assemble test set\n",
    "\n",
    "stratdf_test = test_neg_helpful.append(test_neg_unhelpful, ignore_index=True)\n",
    "stratdf_test = stratdf_test.append(test_pos_unhelpful, ignore_index=True)\n",
    "stratdf_test = stratdf_test.append(test_pos_helpful, ignore_index=True)\n",
    "print(f\"Our test dataset is now {stratdf_test.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stratdf_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a3c566f9ffce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_train_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstratdf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'asin'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prepReviewText'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'group_z_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_dev_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstratdf_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prepReviewText'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'group_z_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_test_prep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstratdf_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prepReviewText'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'group_z_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stratdf_dev' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_prep = shuffle(stratdf_train,random_state=42)[['asin','prepReviewText','group_z_class']]\n",
    "df_dev_prep = shuffle(stratdf_dev,random_state=42)[['prepReviewText','group_z_class']]\n",
    "df_test_prep = shuffle(stratdf_test,random_state=42)[['prepReviewText','group_z_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_asin_list = list(df_train_prep['asin'].unique())\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('group_asin_list.pkl', 'wb') as f:\n",
    "    pickle.dump(group_asin_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prep.to_csv('train_prep_clust_FINAL.csv')\n",
    "df_dev_prep.to_csv('dev_prep_clust_FINAL.csv')\n",
    "df_test_prep.to_csv('test_prep_clust_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the above steps if restarting kernel/notebook and just load the data\n",
    "\n",
    "df_train_prep = pd.read_csv('../w266_proj/data/train_clust_FINAL.csv')\n",
    "df_dev_prep = pd.read_csv('../w266_proj/data/dev_clust_FINAL.csv')\n",
    "df_test_prep = pd.read_csv('../w266_proj/data/test_clust_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prepReviewText</th>\n",
       "      <th>group_z_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11245</td>\n",
       "      <td>WORST This is my least favorite book on breast...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18683</td>\n",
       "      <td>WORST This is a depressing  book.  I like upli...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43825</td>\n",
       "      <td>BEST One of my favorite television programs of...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9253</td>\n",
       "      <td>WORST Aryan Nation, torture, suicide, rape, de...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1165</td>\n",
       "      <td>WORST Chelsea is touted as one of the top come...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     prepReviewText  \\\n",
       "0       11245  WORST This is my least favorite book on breast...   \n",
       "1       18683  WORST This is a depressing  book.  I like upli...   \n",
       "2       43825  BEST One of my favorite television programs of...   \n",
       "3        9253  WORST Aryan Nation, torture, suicide, rape, de...   \n",
       "4        1165  WORST Chelsea is touted as one of the top come...   \n",
       "\n",
       "   group_z_class  \n",
       "0            1.0  \n",
       "1            0.0  \n",
       "2            1.0  \n",
       "3            1.0  \n",
       "4            1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = df_train_prep['prepReviewText']\n",
    "X_dev_prep = df_dev_prep['prepReviewText']\n",
    "X_test_prep = df_test_prep['prepReviewText']\n",
    "\n",
    "y_train_prep = df_train_prep['group_z_class']\n",
    "y_dev_prep = df_dev_prep['group_z_class']\n",
    "y_test_prep = df_test_prep['group_z_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 2110756\n"
     ]
    }
   ],
   "source": [
    "# Transform text examples\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             #tokenizer=Tokenizer,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\*|\\-|\\;|\\:|\\,|\\.\",\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=None)\n",
    "\n",
    "X_train_prep_vec = vectorizer.fit_transform(X_train_prep)\n",
    "X_dev_prep_vec = vectorizer.transform(X_dev_prep)\n",
    "X_test_prep_vec = vectorizer.transform(X_test_prep)\n",
    "\n",
    "print(\"Token Count: {}\".format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "clf = LogisticRegression(penalty='l2',\n",
    "                         C=1.0,\n",
    "                         random_state=42,\n",
    "                         solver='saga',\n",
    "                         multi_class='ovr',\n",
    "                         max_iter=100,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   10.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
       "                   solver='saga', tol=0.0001, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "clf.fit(X_train_prep_vec, y_train_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - 2 Class (cluster) Labels FINAL\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.7738\n",
      "f_1 score (Weighted): 0.7737\n"
     ]
    }
   ],
   "source": [
    "dev_predicted_labels = clf.predict(X_dev_prep_vec)\n",
    "f1_weighted = metrics.f1_score(y_dev_prep, dev_predicted_labels, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_dev_prep, dev_predicted_labels)\n",
    "    \n",
    "print('Logistic Regression Classifer - 2 Class (cluster) Labels FINAL')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.4f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.4f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_helpful     0.7858    0.7527    0.7689      3008\n",
      "     helpful     0.7627    0.7949    0.7784      3008\n",
      "\n",
      "    accuracy                         0.7738      6016\n",
      "   macro avg     0.7743    0.7738    0.7737      6016\n",
      "weighted avg     0.7743    0.7738    0.7737      6016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not_helpful', 'helpful']\n",
    "\n",
    "print(metrics.classification_report(y_dev_prep, dev_predicted_labels, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - 2 Class (cluster) Labels FINAL (TEST SET)\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.7652\n",
      "f_1 score (Weighted): 0.7651\n"
     ]
    }
   ],
   "source": [
    "test_predicted_labels = clf.predict(X_test_prep_vec)\n",
    "f1_weighted = metrics.f1_score(y_test_prep, test_predicted_labels, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_prep, test_predicted_labels)\n",
    "    \n",
    "print('Logistic Regression Classifer - 2 Class (cluster) Labels FINAL (TEST SET)')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.4f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.4f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL MODEL RESULTS ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_helpful     0.7773    0.7433    0.7599      3066\n",
      "     helpful     0.7541    0.7870    0.7702      3066\n",
      "\n",
      "    accuracy                         0.7652      6132\n",
      "   macro avg     0.7657    0.7652    0.7651      6132\n",
      "weighted avg     0.7657    0.7652    0.7651      6132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not_helpful', 'helpful']\n",
    "\n",
    "print(metrics.classification_report(y_test_prep, test_predicted_labels, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTATION/TESTING CODE BLOCKS BELOW\n",
    "\n",
    "\n",
    "## 2-Class Testing With Groups\n",
    "\n",
    "Jen's buckets using random sampling to form 4 balanced groups across 2 classes\n",
    "\n",
    "1. Oversampling all groups produces odd results (equal precision, recall, and f_1 scores across the board)\n",
    "2. Oversampling underrepresented group might improve results\n",
    "3. Balanced groups (11,889) produces best results (~77%)\n",
    "4. Random sampling the 2 classes underperforms using groups (~65.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_neg_helpful: (81152, 21)\n",
      "orig_neg_unhelpful: (13033, 21)\n",
      "orig_pos_unhelpful: (121879, 21)\n",
      "orig_pos_helpful: (316234, 21)\n"
     ]
    }
   ],
   "source": [
    "# First check group membership based on the original outcome variable `most_helpful`\n",
    "# How many examples of each group?\n",
    "print('orig_neg_helpful: {}'.format(mine[(mine.overall == 1) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].shape))\n",
    "print('orig_neg_unhelpful: {}'.format(mine[(mine.overall == 1) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].shape))\n",
    "print('orig_pos_unhelpful: {}'.format(mine[(mine.overall == 5) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].shape))\n",
    "print('orig_pos_helpful: {}'.format(mine[(mine.overall == 5) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl2_neg_helpful: (33416, 21)\n",
      "cl2_neg_unhelpful: (13033, 21)\n",
      "cl2_pos_unhelpful: (121879, 21)\n",
      "cl2_pos_helpful: (167093, 21)\n"
     ]
    }
   ],
   "source": [
    "# Next, check group membership based on clustering on scaled annual_HVAR values\n",
    "# How many examples of each group?\n",
    "print('cl2_neg_helpful: {}'.format(mine[(mine.overall == 1) & (mine.group_z_class == 1) & (mine.helpful_votes != 0)].shape))\n",
    "print('cl2_neg_unhelpful: {}'.format(mine[(mine.overall == 1) & (mine.group_z_class == 0) & (mine.helpful_votes == 0)].shape))\n",
    "print('cl2_pos_unhelpful: {}'.format(mine[(mine.overall == 5) & (mine.group_z_class == 0) & (mine.helpful_votes == 0)].shape))\n",
    "print('cl2_pos_helpful: {}'.format(mine[(mine.overall == 5) & (mine.group_z_class == 1) & (mine.helpful_votes != 0)].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Original and new (cluster) labeling scheme have same quantity of membership for unhelpful reviews**\n",
    "This makes sense. The clustering approach changes the criteria for inclusion in a helpful category (it makes it harder to get labeled as helpful). This means that the extreme (1-star and 5-star) unhelpful cases should still be the same, but extreme helpful cases will be a smaller set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum balanced group size old labels (sampling without replacement) = 11,889**\n",
    "**Maximum balanced group size 2-class (sampling without replacement) = 11,889**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare groups based on original labels\n",
    "# Below I sample to have equal amounts of pos/neg reviews and equal amounts of top-quartile-HVAR vs 0 helpful votes\n",
    "\n",
    "num_per_condition = 13033\n",
    "repl=False\n",
    "orig_neg_helpful = mine[(mine.overall == 1) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "orig_neg_unhelpful = mine[(mine.overall == 1) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "orig_pos_unhelpful = mine[(mine.overall == 5) & (mine.most_helpful == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "orig_pos_helpful = mine[(mine.overall == 5) & (mine.most_helpful == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare groups based on new 2-class labels\n",
    "\n",
    "num_per_condition = 13033\n",
    "repl=False\n",
    "cl2_neg_helpful = mine[(mine.overall == 1) & (mine.group_z_class == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "cl2_neg_unhelpful = mine[(mine.overall == 1) & (mine.group_z_class == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=True, random_state=42)\n",
    "cl2_pos_unhelpful = mine[(mine.overall == 5) & (mine.group_z_class == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "cl2_pos_helpful = mine[(mine.overall == 5) & (mine.group_z_class == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "# \"reviewText\" has the review content\n",
    "# \"most_helpful\" has the label of 0 or 1\n",
    "# \"overall\" has the star-rating {1,2,3,4,5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare groups based on random sample from 2-classes\n",
    "\n",
    "#num_per_condition = 300000\n",
    "#repl=False\n",
    "\n",
    "#cl2_0 = mine[mine.class_2 == 0].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl2_1 = mine[mine.class_2 == 1].sample(num_per_condition, replace=repl, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cl2_0['prepReviewText'] = cl2_0.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "#cl2_1['prepReviewText'] = cl2_1.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with prepending TEXT representation of starts to the reviews, \n",
    "# as a way to pass overall rating to our classifier\n",
    "# because haven't figured out how to send categorical data AROUND the transformer yet\n",
    "orig_neg_helpful['prepReviewText'] = orig_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "orig_neg_unhelpful['prepReviewText'] = orig_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "orig_pos_unhelpful['prepReviewText'] = orig_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "orig_pos_helpful['prepReviewText'] = orig_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with prepending TEXT representation of starts to the reviews, \n",
    "# as a way to pass overall rating to our classifier\n",
    "# because haven't figured out how to send categorical data AROUND the transformer yet\n",
    "cl2_neg_helpful['prepReviewText'] = cl2_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "cl2_neg_unhelpful['prepReviewText'] = cl2_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "cl2_pos_unhelpful['prepReviewText'] = cl2_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "cl2_pos_helpful['prepReviewText'] = cl2_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with 2-class (random) labels is now 600000 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "#stratdf_cl2_rand = cl2_0.append(cl2_1, ignore_index=True)\n",
    "#print(f\"Our dataset with 2-class (random) labels is now {stratdf_cl2_rand.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with original labels is now 52132 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "stratdf_orig = orig_neg_helpful.append(orig_neg_unhelpful, ignore_index=True)\n",
    "stratdf_orig = stratdf_orig.append(orig_pos_unhelpful, ignore_index=True)\n",
    "stratdf_orig = stratdf_orig.append(orig_pos_helpful, ignore_index=True)\n",
    "print(f\"Our dataset with original labels is now {stratdf_orig.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with 2-class labels is now 52132 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "stratdf_cl2 = cl2_neg_helpful.append(cl2_neg_unhelpful, ignore_index=True)\n",
    "stratdf_cl2 = stratdf_cl2.append(cl2_pos_unhelpful, ignore_index=True)\n",
    "stratdf_cl2 = stratdf_cl2.append(cl2_pos_helpful, ignore_index=True)\n",
    "print(f\"Our dataset with 2-class labels is now {stratdf_cl2.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    26066\n",
       "0    26066\n",
       "Name: most_helpful, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balances classes\n",
    "stratdf_orig['most_helpful'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    30937\n",
       "1.0    21195\n",
       "Name: class_2, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balances classes\n",
    "stratdf_cl2['class_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    26066\n",
       "1.0    26066\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balances classes\n",
    "stratdf_cl2['group_z_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>review_age_days</th>\n",
       "      <th>annual_HVAR</th>\n",
       "      <th>book_num_reviews</th>\n",
       "      <th>std_HVAR</th>\n",
       "      <th>top_quartile_HVAR</th>\n",
       "      <th>min_max</th>\n",
       "      <th>most_helpful</th>\n",
       "      <th>scaled</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>prepReviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0812885147</td>\n",
       "      <td>1</td>\n",
       "      <td>Many of the more sensational items in this boo...</td>\n",
       "      <td>2005-08-29</td>\n",
       "      <td>A6GMEO3VRY51S</td>\n",
       "      <td>microjoe</td>\n",
       "      <td>Not a balanced view</td>\n",
       "      <td>1125273600</td>\n",
       "      <td>29</td>\n",
       "      <td>3250</td>\n",
       "      <td>3.256923</td>\n",
       "      <td>10</td>\n",
       "      <td>1.069925</td>\n",
       "      <td>0.276051</td>\n",
       "      <td>(0.0, 3.256923076923077)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WORST Many of the more sensational items in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1608194809</td>\n",
       "      <td>1</td>\n",
       "      <td>Save your money; here are the only observation...</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>A298FZ0IH1Y09R</td>\n",
       "      <td>Roberta Proctor \"Roberta\"</td>\n",
       "      <td>A Little Too Secretive</td>\n",
       "      <td>1325376000</td>\n",
       "      <td>68</td>\n",
       "      <td>934</td>\n",
       "      <td>26.573876</td>\n",
       "      <td>20</td>\n",
       "      <td>10.879657</td>\n",
       "      <td>9.667526</td>\n",
       "      <td>(0.8039647577092511, 36.94891201513718)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WORST Save your money; here are the only obser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0312578075</td>\n",
       "      <td>1</td>\n",
       "      <td>Most authors get better with time and experien...</td>\n",
       "      <td>2012-10-13</td>\n",
       "      <td>A3NORUUG9HYUQY</td>\n",
       "      <td>SSB</td>\n",
       "      <td>Just Wretched!  Big Yawn!</td>\n",
       "      <td>1350086400</td>\n",
       "      <td>8</td>\n",
       "      <td>648</td>\n",
       "      <td>4.506173</td>\n",
       "      <td>11</td>\n",
       "      <td>1.650425</td>\n",
       "      <td>3.210855</td>\n",
       "      <td>(0.0, 4.506172839506172)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WORST Most authors get better with time and ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0385301898</td>\n",
       "      <td>1</td>\n",
       "      <td>Because of good marketing, Lauurie Cabot is pr...</td>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>A26TY6GCGBF5HT</td>\n",
       "      <td>Graves</td>\n",
       "      <td>badly flawed and egotistical.</td>\n",
       "      <td>1206662400</td>\n",
       "      <td>8</td>\n",
       "      <td>2308</td>\n",
       "      <td>1.265165</td>\n",
       "      <td>20</td>\n",
       "      <td>0.723741</td>\n",
       "      <td>0.761366</td>\n",
       "      <td>(0.08130986856760972, 3.0692108667529108)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.396216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WORST Because of good marketing, Lauurie Cabot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0442273118</td>\n",
       "      <td>1</td>\n",
       "      <td>This book disappoints.  The idea of an undergr...</td>\n",
       "      <td>2012-03-10</td>\n",
       "      <td>A2MMTR804CCXYF</td>\n",
       "      <td>Zaur</td>\n",
       "      <td>Title attractive but content disappointing.</td>\n",
       "      <td>1331337600</td>\n",
       "      <td>28</td>\n",
       "      <td>865</td>\n",
       "      <td>11.815029</td>\n",
       "      <td>12</td>\n",
       "      <td>23.419022</td>\n",
       "      <td>3.757260</td>\n",
       "      <td>(0.0, 82.80399274047187)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WORST This book disappoints.  The idea of an u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall                                         reviewText  \\\n",
       "0  0812885147        1  Many of the more sensational items in this boo...   \n",
       "1  1608194809        1  Save your money; here are the only observation...   \n",
       "2  0312578075        1  Most authors get better with time and experien...   \n",
       "3  0385301898        1  Because of good marketing, Lauurie Cabot is pr...   \n",
       "4  0442273118        1  This book disappoints.  The idea of an undergr...   \n",
       "\n",
       "   reviewTime      reviewerID               reviewerName  \\\n",
       "0  2005-08-29   A6GMEO3VRY51S                   microjoe   \n",
       "1  2012-01-01  A298FZ0IH1Y09R  Roberta Proctor \"Roberta\"   \n",
       "2  2012-10-13  A3NORUUG9HYUQY                        SSB   \n",
       "3  2008-03-28  A26TY6GCGBF5HT                     Graves   \n",
       "4  2012-03-10  A2MMTR804CCXYF                       Zaur   \n",
       "\n",
       "                                       summary  unixReviewTime  helpful_votes  \\\n",
       "0                          Not a balanced view      1125273600             29   \n",
       "1                       A Little Too Secretive      1325376000             68   \n",
       "2                    Just Wretched!  Big Yawn!      1350086400              8   \n",
       "3                badly flawed and egotistical.      1206662400              8   \n",
       "4  Title attractive but content disappointing.      1331337600             28   \n",
       "\n",
       "   review_age_days  annual_HVAR  book_num_reviews   std_HVAR  \\\n",
       "0             3250     3.256923                10   1.069925   \n",
       "1              934    26.573876                20  10.879657   \n",
       "2              648     4.506173                11   1.650425   \n",
       "3             2308     1.265165                20   0.723741   \n",
       "4              865    11.815029                12  23.419022   \n",
       "\n",
       "   top_quartile_HVAR                                    min_max  most_helpful  \\\n",
       "0           0.276051                   (0.0, 3.256923076923077)             1   \n",
       "1           9.667526    (0.8039647577092511, 36.94891201513718)             1   \n",
       "2           3.210855                   (0.0, 4.506172839506172)             1   \n",
       "3           0.761366  (0.08130986856760972, 3.0692108667529108)             1   \n",
       "4           3.757260                   (0.0, 82.80399274047187)             1   \n",
       "\n",
       "     scaled  class_2  class_3  \\\n",
       "0  1.000000      1.0      2.0   \n",
       "1  0.712960      1.0      2.0   \n",
       "2  1.000000      1.0      2.0   \n",
       "3  0.396216      0.0      1.0   \n",
       "4  0.142687      0.0      0.0   \n",
       "\n",
       "                                      prepReviewText  \n",
       "0  WORST Many of the more sensational items in th...  \n",
       "1  WORST Save your money; here are the only obser...  \n",
       "2  WORST Most authors get better with time and ex...  \n",
       "3  WORST Because of good marketing, Lauurie Cabot...  \n",
       "4  WORST This book disappoints.  The idea of an u...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratdf_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "df_orig = shuffle(stratdf_orig,random_state=42)[['prepReviewText','overall','most_helpful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class\n",
    "df_cl2 = shuffle(stratdf_cl2,random_state=42)[['prepReviewText','overall','group_z_class']]\n",
    "df_cl2.to_csv('df_cl2_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class random (no groups)\n",
    "#df_cl2_rand = shuffle(stratdf_cl2_rand,random_state=42)[['prepReviewText','overall','class_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Labels\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(df_orig.prepReviewText,df_orig.most_helpful, test_size=0.2, \\\n",
    "                                    random_state=42,stratify=df_orig.most_helpful)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class labels\n",
    "X_train_cl2, X_test_cl2, y_train_cl2, y_test_cl2 = train_test_split(df_cl2.prepReviewText,df_cl2.group_z_class, test_size=0.2, \\\n",
    "                                   random_state=42,stratify=df_cl2.group_z_class)\n",
    "# Ideally I would like to stratify such that train and test have stratified samples across\n",
    "# BOTH the most_helpful values AND the overall rating, but I keep getting errors when I try to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class labels (random)\n",
    "#X_train_cl2_rand, X_test_cl2_rand, y_train_cl2_rand, y_test_cl2_rand = train_test_split(df_cl2_rand.prepReviewText,df_cl2_rand.class_2, test_size=0.2, \\\n",
    "                                   #random_state=42,stratify=df_cl2_rand.class_2)\n",
    "# Ideally I would like to stratify such that train and test have stratified samples across\n",
    "# BOTH the most_helpful values AND the overall rating, but I keep getting errors when I try to do that\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count - 2 class: 1754541\n"
     ]
    }
   ],
   "source": [
    "# Transform text examples\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             #tokenizer=Tokenizer,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\*|\\-|\\;|\\:|\\,|\\.\",\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=None)\n",
    "\n",
    "#X_train_orig_vec = vectorizer.fit_transform(X_train_orig)\n",
    "#X_test_orig_vec = vectorizer.transform(X_test_orig)\n",
    "\n",
    "#print(\"Token Count - Original: {}\".format(len(vectorizer.get_feature_names())))\n",
    "\n",
    "X_train_cl2_vec = vectorizer.fit_transform(X_train_cl2)\n",
    "X_test_cl2_vec = vectorizer.transform(X_test_cl2)\n",
    "\n",
    "print(\"Token Count - 2 class: {}\".format(len(vectorizer.get_feature_names())))\n",
    "\n",
    "\n",
    "\n",
    "#X_train_cl2_rand_vec = vectorizer.fit_transform(X_train_cl2_rand)\n",
    "#X_test_cl2_rand_vec = vectorizer.transform(X_test_cl2_rand)\n",
    "\n",
    "#print(\"Token Count - 2 class: {}\".format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Original Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='ovr', n_jobs=-1, penalty='l2', random_state=42,\n",
       "                   solver='saga', tol=0.0001, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2',\n",
    "                         C=1.0,\n",
    "                         random_state=42,\n",
    "                         solver='saga',\n",
    "                         multi_class='ovr',\n",
    "                         max_iter=100,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train_orig_vec, y_train_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_labels_orig = clf.predict(X_test_orig_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - Original Labels\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.731\n",
      "f_1 score (Weighted): 0.731\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with various f1 metrics\n",
    "f1_weighted = metrics.f1_score(y_test_orig, test_predicted_labels_orig, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_orig, test_predicted_labels_orig)\n",
    "    \n",
    "print('Logistic Regression Classifer - Original Labels')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_helpful       0.74      0.71      0.73      5214\n",
      "     helpful       0.72      0.75      0.74      5213\n",
      "\n",
      "    accuracy                           0.73     10427\n",
      "   macro avg       0.73      0.73      0.73     10427\n",
      "weighted avg       0.73      0.73      0.73     10427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not_helpful', 'helpful']\n",
    "\n",
    "print(metrics.classification_report(y_test_orig, test_predicted_labels_orig, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - 2 Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2',\n",
    "                         C=1.0,\n",
    "                         random_state=42,\n",
    "                         solver='saga',\n",
    "                         multi_class='ovr',\n",
    "                         max_iter=100,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 7 seconds\n",
      "Logistic Regression Classifer - 2 Class (cluster) Labels v5\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.786\n",
      "f_1 score (Weighted): 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.8s finished\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_cl2_vec, y_train_cl2)\n",
    "test_predicted_labels_cl2 = clf.predict(X_test_cl2_vec)\n",
    "f1_weighted = metrics.f1_score(y_test_cl2, test_predicted_labels_cl2, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_cl2, test_predicted_labels_cl2)\n",
    "    \n",
    "print('Logistic Regression Classifer - 2 Class (cluster) Labels v5')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4045 1169]\n",
      " [1063 4150]]\n",
      "True Negative: 4045\n",
      "True Positive: 4150\n",
      "False Negative: 1063\n",
      "False Positive: 1169\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test_cl2, test_predicted_labels_cl2))\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test_cl2, test_predicted_labels_cl2).ravel()\n",
    "print(\"True Negative: {}\".format(tn))\n",
    "print(\"True Positive: {}\".format(tp))\n",
    "print(\"False Negative: {}\".format(fn))\n",
    "print(\"False Positive: {}\".format(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_helpful       0.79      0.78      0.78      5214\n",
      "     helpful       0.78      0.80      0.79      5213\n",
      "\n",
      "    accuracy                           0.79     10427\n",
      "   macro avg       0.79      0.79      0.79     10427\n",
      "weighted avg       0.79      0.79      0.79     10427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_names = ['not_helpful', 'helpful']\n",
    "\n",
    "print(metrics.classification_report(y_test_cl2, test_predicted_labels_cl2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Testing\n",
    "\n",
    "# Here I have a few \"canary in the coal mine\" examples\n",
    "# If the model can't get these right, I put very little faith in it\n",
    "\n",
    "# This is 1* without sharing why\n",
    "vacuous_negative = \"WORST I like to read many different books from many points of view.  This book was a complete waste of time that I will never get back. Save yourself the trouble and money.\"\n",
    "\n",
    "# This is 5* but worthless\n",
    "vacuous_positive = \"BEST My husband has really loved this series and I have heard the same comments from many others who have also read this series.\"\n",
    "\n",
    "# This looks like someone who didn't read the book was paid to write a long 5* review\n",
    "# it sounds like a completely generic description of any cookbook (\"it provides recipes to prepare foods...\")\n",
    "# Perhaps mentions of many concepts (ingredients, gourmet, etc.) can fool some people and also an algorithm\n",
    "# Helpful votes: 71\n",
    "# Annual HVAR: 5\n",
    "# For this book, the top quartile was HVAR of 1.7\n",
    "# Surely adding GENRE would knock this one down\n",
    "vacuous_cookbook = \"BEST As someone who is learning to cook only late in her life, I was apprehensive and embarrassed about asking simple basic questions of friends and family.  Perceiving this, my parents gave me this cookbook, and voila!  -- I can cook!With step-by-step instructions on everything from  cookware, ingredients, buying, preapring, cooking, and serving, there's  nothing this book can't handle.  It provides recipes to prepare foods in  the simplest ways, all the way up to complex gourmet dishes.  And it covers  every imaginable food -- if it isn't in here, I can't imagine where you'd  find it.The language is straightforward and encouraging, with  appropriate editorializing on the author's preferences, and the layout is  clean and easy to read.  I can't say enough good things about this cookbook  -- it never leaves my kitchen counter.\"\n",
    "\n",
    "# Reviewer hasn't read it yet\n",
    "vacuous_not_read = \"BEST Just downloaded this series.  Looking forward to getting to read it, once i get past some of the other books on my reading list.  I just love beig able to carryall these books without having to carry them individually!\"\n",
    "\n",
    "# This excerpt of a review got 5* and JenD finds it helpful \n",
    "meaty_positive = \"BEST After \\\"Riding Lessons\\\", which I loved - and \\\"Flying Changes\\\", which was a huge disappointment to me, I was not sure what I would find in \\\"Water for Elephants\\\".  Wow - what a great read!  Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara.  This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Reading \\\"Water for Elephants\\\" is time well-spent.  I'm happy to know Sara is working on a fourth book. I'll be first in line to buy it.\"\n",
    "#Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara. This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Time well spent!\"\n",
    "\n",
    "# Another excerpt from a 5* with many helpful reviews\n",
    "meaty_positive2 = \"BEST This had the flavor of The Great Gatsby, the well to do characters spend their days going from luncheon to evening parties and everyone is concerned about who's who. That is, on the surface it has that flavor. Beneath is a gripping story more about Trudy and Will than about the piano teacher, Claire and the desperate desire to survive.The characters in this book are well developed. There is a lot going on beneath the surface that the author lets you discern.Life in Hong Kong during the 40's is a lark and all about the parties you go to, until the Japanese occupation.  Will is interned along with many of the other socialites.  Life becomes getting food, keeping warm, keeping from being infested, keeping from being singled out for abuse.\"\n",
    "\n",
    "# This excerpt is from a negative 1* review and includes arguments (>300 helpful votes) \n",
    "meaty_negative = \"WORST Colin Campbell is so intent on promoting a vegan data that he misrepresents the data in the real China Study and cherry picks anti-animal food data. For instance, he rightly cites the link between milk and autoimmune disease but fails to mention that gluten, from wheat and related grains, is at least as important a cause. He writes of the association between casein, a milk protein, with cancer, but fails to mention that whey and butterfat are protective against cancer, and in milk you get all of them. He makes completely false statements like folate not being in meat when organ meats are much higher in folate than any plant source according to the USDA. He assumes nutrient consistency with the US without actually measuring it, despite the fact that soil nutrients and species differences have a huge effect on nutrition.\"\n",
    "\n",
    "# This is a scholar reviewing another scholar's 5* work\n",
    "# Helpful votes: 788 and annual HVAR: 74\n",
    "meaty_scholar = \"BEST Noted historian of the early church Elaine Pagels has produced a clear, cogent, and very effective introduction to the subject of Gnosticism, a different form of Christianity that was declared heretical and virtually stamped out by the orthodox church by the start of the second century after Christ.  Most of what we knew of the Gnostic belief system came from the religious authors who worked so hard to destroy the movement, but that changed drastically with the still relatively recent discovery of a number of lost Gnostic writings near Nag Hammadi in Upper Egypt.  Unlike the Dead Sea Scrolls, this momentous discovery of ancient papyri has received little attention, and I must admit I went into this book knowing virtually nothing about Gnosticism.  As an historian by training and a Christian, the information in these &quot;heretical&quot; texts intrigue me, and I believe that Christians should challenge their faith by examining material that does not fall in line\"\n",
    "\n",
    "# Make the list of canaries\n",
    "pred_sentences = [vacuous_negative,\\\n",
    "                  vacuous_positive, \\\n",
    "                  vacuous_cookbook, \\\n",
    "                  vacuous_not_read, \\\n",
    "                  meaty_negative, \\\n",
    "                  meaty_positive, \\\n",
    "                  meaty_positive2, \\\n",
    "                  meaty_scholar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "canaries = vectorizer.transform(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canaries_pred = clf.predict(canaries)\n",
    "canaries_pred_prob = clf.predict_proba(canaries)\n",
    "\n",
    "for i in canaries_pred:\n",
    "    print(i)\n",
    "\n",
    "canaries_pred\n",
    "np.argmax(canaries_pred_prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gee...not really sure\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "Gee...not really sure\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for pred in canaries_pred_prob:\n",
    "    max_class_prob = max(pred)\n",
    "    \n",
    "    boundary_dist = max_class_prob - 0.5\n",
    "    \n",
    "    if boundary_dist < 0.1:\n",
    "        print('Gee...not really sure')\n",
    "    else:\n",
    "        print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             #tokenizer=Tokenizer,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\*|\\-|\\;|\\:|\\,|\\.\",\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=None)\n",
    "\n",
    "X_train_cl2_vec = vectorizer.fit_transform(X_train_cl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [\"Unhelpful\", \"Helpful\"]\n",
    "    \n",
    "    tfidf_transform = vectorizer.transform(in_sentences)\n",
    "    \n",
    "    predictions = clf.predict_proba(tfidf_transform)\n",
    "    \n",
    "    return [(sentence, prediction, labels[np.argmax(prediction)]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WORST I like to read many different books from many points of view.  This book was a complete waste of time that I will never get back. Save yourself the trouble and money.',\n",
       "  array([0.5854154, 0.4145846]),\n",
       "  'Unhelpful'),\n",
       " ('BEST My husband has really loved this series and I have heard the same comments from many others who have also read this series.',\n",
       "  array([0.86454695, 0.13545305]),\n",
       "  'Unhelpful'),\n",
       " (\"BEST As someone who is learning to cook only late in her life, I was apprehensive and embarrassed about asking simple basic questions of friends and family.  Perceiving this, my parents gave me this cookbook, and voila!  -- I can cook!With step-by-step instructions on everything from  cookware, ingredients, buying, preapring, cooking, and serving, there's  nothing this book can't handle.  It provides recipes to prepare foods in  the simplest ways, all the way up to complex gourmet dishes.  And it covers  every imaginable food -- if it isn't in here, I can't imagine where you'd  find it.The language is straightforward and encouraging, with  appropriate editorializing on the author's preferences, and the layout is  clean and easy to read.  I can't say enough good things about this cookbook  -- it never leaves my kitchen counter.\",\n",
       "  array([0.11067897, 0.88932103]),\n",
       "  'Helpful'),\n",
       " ('BEST Just downloaded this series.  Looking forward to getting to read it, once i get past some of the other books on my reading list.  I just love beig able to carryall these books without having to carry them individually!',\n",
       "  array([0.82734487, 0.17265513]),\n",
       "  'Unhelpful'),\n",
       " ('WORST Colin Campbell is so intent on promoting a vegan data that he misrepresents the data in the real China Study and cherry picks anti-animal food data. For instance, he rightly cites the link between milk and autoimmune disease but fails to mention that gluten, from wheat and related grains, is at least as important a cause. He writes of the association between casein, a milk protein, with cancer, but fails to mention that whey and butterfat are protective against cancer, and in milk you get all of them. He makes completely false statements like folate not being in meat when organ meats are much higher in folate than any plant source according to the USDA. He assumes nutrient consistency with the US without actually measuring it, despite the fact that soil nutrients and species differences have a huge effect on nutrition.',\n",
       "  array([0.22560673, 0.77439327]),\n",
       "  'Helpful'),\n",
       " ('BEST After \"Riding Lessons\", which I loved - and \"Flying Changes\", which was a huge disappointment to me, I was not sure what I would find in \"Water for Elephants\".  Wow - what a great read!  Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara.  This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Reading \"Water for Elephants\" is time well-spent.  I\\'m happy to know Sara is working on a fourth book. I\\'ll be first in line to buy it.',\n",
       "  array([0.50460479, 0.49539521]),\n",
       "  'Unhelpful'),\n",
       " (\"BEST This had the flavor of The Great Gatsby, the well to do characters spend their days going from luncheon to evening parties and everyone is concerned about who's who. That is, on the surface it has that flavor. Beneath is a gripping story more about Trudy and Will than about the piano teacher, Claire and the desperate desire to survive.The characters in this book are well developed. There is a lot going on beneath the surface that the author lets you discern.Life in Hong Kong during the 40's is a lark and all about the parties you go to, until the Japanese occupation.  Will is interned along with many of the other socialites.  Life becomes getting food, keeping warm, keeping from being infested, keeping from being singled out for abuse.\",\n",
       "  array([0.26395058, 0.73604942]),\n",
       "  'Helpful'),\n",
       " ('BEST Noted historian of the early church Elaine Pagels has produced a clear, cogent, and very effective introduction to the subject of Gnosticism, a different form of Christianity that was declared heretical and virtually stamped out by the orthodox church by the start of the second century after Christ.  Most of what we knew of the Gnostic belief system came from the religious authors who worked so hard to destroy the movement, but that changed drastically with the still relatively recent discovery of a number of lost Gnostic writings near Nag Hammadi in Upper Egypt.  Unlike the Dead Sea Scrolls, this momentous discovery of ancient papyri has received little attention, and I must admit I went into this book knowing virtually nothing about Gnosticism.  As an historian by training and a Christian, the information in these &quot;heretical&quot; texts intrigue me, and I believe that Christians should challenge their faith by examining material that does not fall in line',\n",
       "  array([0.23398814, 0.76601186]),\n",
       "  'Helpful')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = getPrediction(pred_sentences)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['wonderful best favorite excellent' for x in range(500)]\n",
    "test = ' '.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_help_rev_pos_1 = \"I've had this iPad for just a few weeks but I love it. It arrived brand new in original packaging with all the standard accessories. I bought the iPad for the sole purpose of using it to start getting into digital art. When I say sole purpose, I mean I only use this iPad for ProCreate. I never have the internet on or use it for videos since I use my MacAir or iPhone for those things. I purchased the Apple Pencil and it works wonderfully on this iPad. For anyone who is interested in this more budget iPad for digital art or taking notes, I would recommend it. I bought a case for the iPad and it isn't as light as you might think. It has some weight to it. So if you are someone who really needs a REALLY lightweight tablet, this might not be great. The battery life so far is amazing. But, like I said before I don't have the internet on or any other app running besides ProCreate when I am using it. Maybe in two years this little guy won't be running so great, but for right now I am so pleased with my purchase. The one thing I might regret is not buying the one with more GB of memory. If you are looking into breaking into digital art, or would like to start doing digital illustrations, I would recommend this. However, for someone who has a good grasp of digital art or has used a previous tablet, consider the size of this tablet. If you need a lot of space to draw, this screen size might be too small and you should consider the 10 or 12 inch ipad Pro line. I have small hands and haven't ever used a tablet to draw before so it works great for me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_help_rev_pos_low_votes = \"Much faster and less glitchy than my Air 2, battery lasts longer, no longer have trouble finding nearby networks. I know people said its not worth the upgrade if you have an Air 2 but it was totally worth it to me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_help_rev_pos_more_votes = \"Ordered the new iPad because the sale was too good to let go. I had wanted the iPad Apple Pencil pair, so being able to get both for the price of just the iPad after tax was worth it. Apple got us, yet again. Not that I mind though, I love my apple products. This iPad is a great example. I will primarily be using it for digital art/graphic design/photo editing adobe type programs. Ive used half the Wacom models, including the cintique. Havent had a chance to use the new iPad Pro and pencil, but for the more affordable version, this earns all 5 stars. #1, gorgeous screen, easy to set up, easy to use right out of the box. Set up is even easier now if you have another apple device. The setup prompt me to grab an apple device logged into my account already. From there I entered my passcode on my phone, before I knew it the iPad was asking if I wanted all my apps brought over, or if I wanted to start fresh. They took what I thought was already a pretty easy process thanks to iCloud, and made it even more user friendly. #2, it feels like the money you spent. Unlike other android devices I have attempted, iOS devices have never let me down in the quality department. This being said, its not as light as the kindle fire or what have you, but its going to last longer and be a more enjoyable experience over all for the extra couple of ounces. Its not heavy compared to my MacBook, which is why I have it. Fits in my purse while still being a gorgeous screen to watch movies on. Retina displays will change your life, and ruin every other screen you look at. #3, the Apple Pencil. I get it now. And do not regret a single penny spent. The pressure sensitivity, accuracy of the accelerometer, how fast it charges, and how the pen to screen experience is closer to paper than anything Ive ever used. Im talking Wacom Cintique level accuracy on a tablet. It feels unreal. I love my moleskines more than the next person, but Im worried they might be getting less use in the coming months just because this is so user friendly. Ill be sure to add battery details or any updates, but those 3 alone were enough to sell me on the purchase.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_help_rev_neg_1 = \"I bought this iPad as Christmas gift for myself and together with the apple pencil I was planning on using them for a more productive semester at school. One day I put this iPad on the table(about 1.65 foot tall) and it dropped shortly after I left because of imbalance. I didn't even think what was about to happen as I used my iPad Air(first Generation) for years and it dropped multiple times from place much higher than this table. When I went check my iPad, I was shocked, the entire screen was broken, the tiny glass pieces falling off and cut my finger. I couldn't believe this screen being this fragile, if this wasn't me but a young kid I couldn't able to imagine how bad things could turn to. Apple shouldn't allow its product to has screen this fragile, it's definitely a safety concerns for all its consumers. I called Apple and hope they can help me, the people whom answered my phone call keep telling me there is nothing they can do, and my only option is to replace my screen for 249.99 (yes, exactly how much I got this iPad). This whole thing is so ridiculous, I've never seen a electronic product being this fragile. For all of you thinking about buying this product, please make sure you got yourself a case, for your own safety.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_help_pos = \"I wanted to write a review, not just about these headphones, but comparing them to the G4ME ZERO. I originally purchased the G4ME ONE, and upon waking up the next day, realized I bought the wrong ones. I wanted the closed version. So I ordered the ZEROs right away, and within a couple days I had $400 worth of headphones sitting on my desk. First off, both sound absolutely fantastic paired with an adequate amplifier. I gave the ZEROs a good test run with a couple long gaming sessions and listening to some tunes. I was impressed by the quality of the sound, they sounded great. Gaming is where I ran into trouble. I typically use closed headphones, but I've always used cheaper ones, which don't isolate the sound as well as higher quality headphones. Games sounded great, but talking to teammates was very troublesome. They are so well isolated, I can't hear myself talking. At all. Plug your ears with your fingers and try holding a conversation with someone, exactly like that. Even enabling the stereo mix setting that allows you to quietly hear yourself through the mic, it wasn't enough. I just could not stand it. Enter the G4ME ONE. I finally unboxed my G4ME ONEs today and plugged them in. The sound quality is close on both, but these seem to be a bit cleaner, especially on the lows. On the ZEROs, bass felt ever so slightly 'detached' from the mids, if that makes any sense. On the ONEs, the frequencies seem to blend better. They're outstanding. Mids and highs are crystal clear, bass is clean and punchy but not boomy at the extreme low end. They sound very natural all around. One small con about both headsets, is that the volume wheel on the right ear cup doesn't go to 0 volume. It only goes down to about 25% volume, which is a little irritating if you want to quickly silence your headset. A few other differences between the two is that the ONEs are an all-plastic design, whereas the ZEROs have a metal rod connecting the ear cups to the headband. How much that actually adds to the structural integrity, I don't know. The ZEROs also come in a carrying case (that nobody in their right mind would use) and so the ear cups rotate 90 degrees to fit into the case. The ONEs only rotate a few degrees, which is fine for me. The ZEROs win hands down in comfort; it's like soft, leather-wrapped pillows around your ears, and they're large, but my ears got very hot very fast (and I was wearing them in a ~55-60 degree house). The ONEs have slightly smaller felt cups and are a little denser but no temperature issues. Other than that, they're very similar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test='Call me Ishmael. Some years agonever mind how long preciselyhaving little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking peoples hats offthen, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call me Ishmael. Some years agonever mind how long preciselyhaving little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking peoples hats offthen, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.',\n",
       "  array([0.32616825, 0.67383175]),\n",
       "  'Helpful')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPrediction([test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomeks removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class Tomek links removed\n",
    "# current winner: ratio='majority'\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority', n_jobs=-1)\n",
    "X_tl, y_tl, id_tl = tl.fit_sample(vectorizer.fit_transform(df_cl2['prepReviewText']), df_cl2['class_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "# 2-class labels\n",
    "X_train_cl2_tl, X_test_cl2_tl, y_train_cl2_tl, y_test_cl2_tl = train_test_split(X_tl,y_tl, test_size=0.2, \\\n",
    "                                   random_state=42,stratify=y_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 6 seconds\n",
      "Logistic Regression Classifer - 2 Class (cluster) Labels\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.760\n",
      "f_1 score (Weighted): 0.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.4s finished\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_cl2_tl, y_train_cl2_tl)\n",
    "test_predicted_labels_cl2_tl = clf.predict(X_test_cl2_tl)\n",
    "f1_weighted = metrics.f1_score(y_test_cl2_tl, test_predicted_labels_cl2_tl, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_cl2_tl, test_predicted_labels_cl2_tl)\n",
    "    \n",
    "print('Logistic Regression Classifer - 2 Class (cluster) Labels')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " not_helpful       0.75      0.77      0.76      5112\n",
      "     helpful       0.77      0.75      0.76      5213\n",
      "\n",
      "    accuracy                           0.76     10325\n",
      "   macro avg       0.76      0.76      0.76     10325\n",
      "weighted avg       0.76      0.76      0.76     10325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not_helpful', 'helpful']\n",
    "\n",
    "print(metrics.classification_report(y_test_cl2_tl, test_predicted_labels_cl2_tl, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WORST I like to read many different books from many points of view.  This book was a complete waste of time that I will never get back. Save yourself the trouble and money.',\n",
       "  array([0.61822559, 0.38177441]),\n",
       "  'Unhelpful'),\n",
       " ('BEST My husband has really loved this series and I have heard the same comments from many others who have also read this series.',\n",
       "  array([0.87253292, 0.12746708]),\n",
       "  'Unhelpful'),\n",
       " (\"BEST As someone who is learning to cook only late in her life, I was apprehensive and embarrassed about asking simple basic questions of friends and family.  Perceiving this, my parents gave me this cookbook, and voila!  -- I can cook!With step-by-step instructions on everything from  cookware, ingredients, buying, preapring, cooking, and serving, there's  nothing this book can't handle.  It provides recipes to prepare foods in  the simplest ways, all the way up to complex gourmet dishes.  And it covers  every imaginable food -- if it isn't in here, I can't imagine where you'd  find it.The language is straightforward and encouraging, with  appropriate editorializing on the author's preferences, and the layout is  clean and easy to read.  I can't say enough good things about this cookbook  -- it never leaves my kitchen counter.\",\n",
       "  array([0.11019787, 0.88980213]),\n",
       "  'Helpful'),\n",
       " ('BEST Just downloaded this series.  Looking forward to getting to read it, once i get past some of the other books on my reading list.  I just love beig able to carryall these books without having to carry them individually!',\n",
       "  array([0.84654782, 0.15345218]),\n",
       "  'Unhelpful'),\n",
       " ('WORST Colin Campbell is so intent on promoting a vegan data that he misrepresents the data in the real China Study and cherry picks anti-animal food data. For instance, he rightly cites the link between milk and autoimmune disease but fails to mention that gluten, from wheat and related grains, is at least as important a cause. He writes of the association between casein, a milk protein, with cancer, but fails to mention that whey and butterfat are protective against cancer, and in milk you get all of them. He makes completely false statements like folate not being in meat when organ meats are much higher in folate than any plant source according to the USDA. He assumes nutrient consistency with the US without actually measuring it, despite the fact that soil nutrients and species differences have a huge effect on nutrition.',\n",
       "  array([0.21048587, 0.78951413]),\n",
       "  'Helpful'),\n",
       " ('BEST After \"Riding Lessons\", which I loved - and \"Flying Changes\", which was a huge disappointment to me, I was not sure what I would find in \"Water for Elephants\".  Wow - what a great read!  Research does pay off hugely - when it enables a writer to place the reader inside another world so easily - the world of the circus. This was a world totally foreign to most of us - but now, so familiar, thanks to Sara.  This book satisfied my three requirements - transportation (take me away from all this),  levitation (lift my spirits and leave me thinking good thoughts) and infiltration (let me get inside the characters so I feel I really know them). Reading \"Water for Elephants\" is time well-spent.  I\\'m happy to know Sara is working on a fourth book. I\\'ll be first in line to buy it.',\n",
       "  array([0.49861352, 0.50138648]),\n",
       "  'Helpful'),\n",
       " (\"BEST This had the flavor of The Great Gatsby, the well to do characters spend their days going from luncheon to evening parties and everyone is concerned about who's who. That is, on the surface it has that flavor. Beneath is a gripping story more about Trudy and Will than about the piano teacher, Claire and the desperate desire to survive.The characters in this book are well developed. There is a lot going on beneath the surface that the author lets you discern.Life in Hong Kong during the 40's is a lark and all about the parties you go to, until the Japanese occupation.  Will is interned along with many of the other socialites.  Life becomes getting food, keeping warm, keeping from being infested, keeping from being singled out for abuse.\",\n",
       "  array([0.25158225, 0.74841775]),\n",
       "  'Helpful'),\n",
       " ('BEST Noted historian of the early church Elaine Pagels has produced a clear, cogent, and very effective introduction to the subject of Gnosticism, a different form of Christianity that was declared heretical and virtually stamped out by the orthodox church by the start of the second century after Christ.  Most of what we knew of the Gnostic belief system came from the religious authors who worked so hard to destroy the movement, but that changed drastically with the still relatively recent discovery of a number of lost Gnostic writings near Nag Hammadi in Upper Egypt.  Unlike the Dead Sea Scrolls, this momentous discovery of ancient papyri has received little attention, and I must admit I went into this book knowing virtually nothing about Gnosticism.  As an historian by training and a Christian, the information in these &quot;heretical&quot; texts intrigue me, and I believe that Christians should challenge their faith by examining material that does not fall in line',\n",
       "  array([0.20842656, 0.79157344]),\n",
       "  'Helpful')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = getPrediction(pred_sentences)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Values for C - 2 Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 7 seconds\n",
      "Best value for C: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.8s finished\n"
     ]
    }
   ],
   "source": [
    "clf_2 = LogisticRegression(penalty='l2',\n",
    "                           C=1.0,\n",
    "                           random_state=42,\n",
    "                           solver='saga',\n",
    "                           multi_class='ovr',\n",
    "                           max_iter=100,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True)\n",
    "\n",
    "param_grid = {'C':list(np.linspace(0.1,1.0,10))}            \n",
    "clf_2 = GridSearchCV(clf_2, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=True)\n",
    "clf_2.fit(X_train_cl2_tl, y_train_cl2_tl)\n",
    "best_c = round(clf_2.best_params_['C'],2)\n",
    "print('Best value for C: {}'.format(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit a model with the best C value\n",
    "clf_2 = LogisticRegression(penalty='l2',\n",
    "                           C=best_c,\n",
    "                           random_state=42,\n",
    "                           solver='saga',\n",
    "                           multi_class='ovr',\n",
    "                           max_iter=100,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True)\n",
    "\n",
    "clf_2.fit(X_train_cl2_tl, y_train_cl2_tl)\n",
    "test_predicted_labels_cl2_tl = clf_2.predict(X_test_cl2_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - 2 Class (cluster) Labels\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.760\n",
      "f_1 score (Weighted): 0.760\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with various f1 metrics\n",
    "f1_weighted = metrics.f1_score(y_test_cl2_tl, test_predicted_labels_cl2_tl, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_cl2_tl, test_predicted_labels_cl2_tl)\n",
    "    \n",
    "print('Logistic Regression Classifer - 2 Class (cluster) Labels')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 3968\n",
      "True Positive: 4075\n",
      "False Negative: 1138\n",
      "False Positive: 1246\n"
     ]
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test_cl2, test_predicted_labels_cl2)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test_cl2, test_predicted_labels_cl2).ravel()\n",
    "print(\"True Negative: {}\".format(tn))\n",
    "print(\"True Positive: {}\".format(tp))\n",
    "print(\"False Negative: {}\".format(fn))\n",
    "print(\"False Positive: {}\".format(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - 3 Class Labels\n",
    "1. Trying to put middle class into buckets produces poor results (f_1 ~ 51%); likely too much variance\n",
    "2. Randomly sampling from entire middle class is only slightly better (f_1 ~ 59.6%)\n",
    "3. Randomly dampling all classes (no buckets) terrible performance (f_1 ~ 44.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl3_neg_helpful: (36977, 19)\n",
      "cl3_neg_unhelpful: (11889, 19)\n",
      "cl3_neg_middle: (20698, 19)\n",
      "cl3_pos_unhelpful: (116227, 19)\n",
      "cl3_pos_helpful: (193895, 19)\n",
      "cl3_pos_middle: (141347, 19)\n"
     ]
    }
   ],
   "source": [
    "# Next, check group membership based on clustering on scaled annual_HVAR values\n",
    "# How many examples of each group?\n",
    "print('cl3_neg_helpful: {}'.format(mine[(mine.overall == 1) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].shape))\n",
    "print('cl3_neg_unhelpful: {}'.format(mine[(mine.overall == 1) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].shape))\n",
    "# a negaitve middle ground review has accumulated SOME helpfulness\n",
    "print('cl3_neg_middle: {}'.format(mine[(mine.overall == 1) & (mine.class_3 == 2) & (mine.helpful_votes != 0)].shape))\n",
    "\n",
    "print('cl3_pos_unhelpful: {}'.format(mine[(mine.overall == 5) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].shape))\n",
    "print('cl3_pos_helpful: {}'.format(mine[(mine.overall == 5) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].shape))\n",
    "# a positive middle ground review has accumulated SOME helpfulness\n",
    "print('cl3_pos_middle: {}'.format(mine[(mine.overall == 5) & (mine.class_3 == 2) & (mine.helpful_votes != 0)].shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare groups based on new 3-class labels (poor results)\n",
    "\n",
    "#num_per_condition = 11889\n",
    "#repl=False\n",
    "#cl3_neg_helpful = mine[(mine.overall == 1) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_neg_unhelpful = mine[(mine.overall == 1) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_neg_middle = mine[(mine.overall == 1) & (mine.class_3 == 2) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "\n",
    "#cl3_pos_unhelpful = mine[(mine.overall == 5) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_pos_helpful = mine[(mine.overall == 5) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_pos_middle = mine[(mine.overall == 5) & (mine.class_3 == 2) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "# \"reviewText\" has the review content\n",
    "# \"most_helpful\" has the label of 0 or 1\n",
    "# \"overall\" has the star-rating {1,2,3,4,5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a random sample from entire middle class (poor results)\n",
    "\n",
    "#num_per_condition = 11889\n",
    "#repl=False\n",
    "#cl3_neg_helpful = mine[(mine.overall == 1) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_neg_unhelpful = mine[(mine.overall == 1) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "\n",
    "#cl3_pos_unhelpful = mine[(mine.overall == 5) & (mine.class_3 == 0) & (mine.helpful_votes == 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "#cl3_pos_helpful = mine[(mine.overall == 5) & (mine.class_3 == 1) & (mine.helpful_votes != 0)].sample(num_per_condition, replace=repl, random_state=42)\n",
    "\n",
    "# two strategies:\n",
    "# - some accumulated helpful votes\n",
    "# - random\n",
    "#cl3_middle = mine[(mine.class_3 == 2) & (mine.helpful_votes != 0)].sample(num_per_condition*1, replace=repl, random_state=42)\n",
    "#cl3_middle = mine[mine.class_3 == 2].sample(num_per_condition*2, replace=repl, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1820423\n",
       "1.0     394950\n",
       "2.0     261174\n",
       "Name: class_3, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine['class_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_condition = 100000\n",
    "repl=False\n",
    "cl3_0 = mine[mine.class_3 == 0].sample(num_per_condition, replace=repl, random_state=42)\n",
    "cl3_1 = mine[mine.class_3 == 1].sample(num_per_condition, replace=repl, random_state=42)\n",
    "cl3_2 = mine[mine.class_3 == 2].sample(num_per_condition, replace=repl, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with prepending TEXT representation of starts to the reviews, \n",
    "# as a way to pass overall rating to our classifier\n",
    "# because haven't figured out how to send categorical data AROUND the transformer yet\n",
    "#cl3_neg_helpful['prepReviewText'] = cl3_neg_helpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "#cl3_neg_unhelpful['prepReviewText'] = cl3_neg_unhelpful.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "#cl3_neg_middle['prepReviewText'] = cl3_neg_middle.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "\n",
    "#cl3_pos_unhelpful['prepReviewText'] = cl3_pos_unhelpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "#cl3_pos_helpful['prepReviewText'] = cl3_pos_helpful.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "#cl3_pos_middle['prepReviewText'] = cl3_pos_middle.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)\n",
    "\n",
    "#cl3_middle['prepReviewText'] = cl3_middle.apply(lambda x: 'BEST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl3_0['prepReviewText'] = cl3_0.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "cl3_1['prepReviewText'] = cl3_1.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)\n",
    "cl3_2['prepReviewText'] = cl3_2.apply(lambda x: 'WORST ' + x.reviewText,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with 3-class labels is now 23778 reviews.\n",
      "Our dataset with 3-class labels is now 35667 reviews.\n",
      "Our dataset with 3-class labels is now 47556 reviews.\n",
      "Our dataset with 3-class labels is now 59445 reviews.\n",
      "Our dataset with 3-class labels is now 71334 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "#stratdf_cl3 = cl3_neg_helpful.append(cl3_neg_unhelpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_pos_unhelpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_pos_helpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_neg_middle, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_pos_middle, ignore_index=True)\n",
    "\n",
    "#print(f\"Our dataset with 3-class labels is now {stratdf_cl3.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with 3-class labels is now 71334 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Put the subsets into the same dataframe again\n",
    "\n",
    "#stratdf_cl3 = cl3_neg_helpful.append(cl3_neg_unhelpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_pos_unhelpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_pos_helpful, ignore_index=True)\n",
    "#stratdf_cl3 = stratdf_cl3.append(cl3_middle, ignore_index=True)\n",
    "\n",
    "#print(f\"Our dataset with 3-class labels is now {stratdf_cl3.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset with 3-class labels is now 300000 reviews.\n"
     ]
    }
   ],
   "source": [
    "stratdf_cl3 = cl3_0.append(cl3_1, ignore_index=True)\n",
    "stratdf_cl3 = stratdf_cl3.append(cl3_2, ignore_index=True)\n",
    "\n",
    "print(f\"Our dataset with 3-class labels is now {stratdf_cl3.shape[0]} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    100000\n",
       "2.0    100000\n",
       "0.0    100000\n",
       "Name: class_3, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balances classes\n",
    "stratdf_cl3['class_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl3 = shuffle(stratdf_cl3,random_state=42)[['prepReviewText','overall','class_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-class labels\n",
    "X_train_cl3, X_test_cl3, y_train_cl3, y_test_cl3 = train_test_split(df_cl3.prepReviewText,df_cl3.class_3, test_size=0.2, \\\n",
    "                                   random_state=42,stratify=df_cl3.class_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count - 3 class: 6893652\n"
     ]
    }
   ],
   "source": [
    "# Transform text examples\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             #tokenizer=Tokenizer,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\*|\\-|\\;|\\:|\\,|\\.\",\n",
    "                             ngram_range=(1,2),\n",
    "                             max_features=None)\n",
    "\n",
    "X_train_cl3_vec = vectorizer.fit_transform(X_train_cl3)\n",
    "X_test_cl3_vec = vectorizer.transform(X_test_cl3)\n",
    "\n",
    "print(\"Token Count - 3 class: {}\".format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 86 seconds\n",
      "convergence after 22 epochs took 93 seconds\n",
      "convergence after 24 epochs took 99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - 3 Class (cluster) Labels\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.457\n",
      "f_1 score (Weighted): 0.445\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_cl3_vec, y_train_cl3)\n",
    "test_predicted_labels_cl3 = clf.predict(X_test_cl3_vec)\n",
    "f1_weighted = metrics.f1_score(y_test_cl3, test_predicted_labels_cl3, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_cl3, test_predicted_labels_cl3)\n",
    "    \n",
    "print('Logistic Regression Classifer - 3 Class (cluster) Labels')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3408  595  752]\n",
      " [1389 1165 2202]\n",
      " [ 839  949 2968]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-3a3c414ac27e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_cl3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_predicted_labels_cl3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_cl3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_predicted_labels_cl3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"True Negative: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"True Positive: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"False Negative: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test_cl3, test_predicted_labels_cl3))\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test_cl3, test_predicted_labels_cl3).ravel()\n",
    "print(\"True Negative: {}\".format(tn))\n",
    "print(\"True Positive: {}\".format(tp))\n",
    "print(\"False Negative: {}\".format(fn))\n",
    "print(\"False Positive: {}\".format(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Correct and Incorrect Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When classifications are incorrect, what are the class probabilities that resulted in class assignment?\n",
    "\n",
    "pred_prob = clf_2.predict_proba(X_test_cl2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65465361, 0.34534639],\n",
       "       [0.74181191, 0.25818809],\n",
       "       [0.61751734, 0.38248266],\n",
       "       [0.44185769, 0.55814231],\n",
       "       [0.24397539, 0.75602461],\n",
       "       [0.2921176 , 0.7078824 ],\n",
       "       [0.74290291, 0.25709709],\n",
       "       [0.84839834, 0.15160166],\n",
       "       [0.03871746, 0.96128254],\n",
       "       [0.22270836, 0.77729164],\n",
       "       [0.1638336 , 0.8361664 ],\n",
       "       [0.34632303, 0.65367697],\n",
       "       [0.19883285, 0.80116715],\n",
       "       [0.2823253 , 0.7176747 ],\n",
       "       [0.30060976, 0.69939024],\n",
       "       [0.23455909, 0.76544091],\n",
       "       [0.45543343, 0.54456657],\n",
       "       [0.85395959, 0.14604041],\n",
       "       [0.85331909, 0.14668091],\n",
       "       [0.47681776, 0.52318224],\n",
       "       [0.03183905, 0.96816095],\n",
       "       [0.84278728, 0.15721272],\n",
       "       [0.12928942, 0.87071058],\n",
       "       [0.88353061, 0.11646939],\n",
       "       [0.86053012, 0.13946988],\n",
       "       [0.14695511, 0.85304489]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23267    0.0\n",
       "23962    0.0\n",
       "21392    0.0\n",
       "44106    1.0\n",
       "42631    1.0\n",
       "37555    1.0\n",
       "29978    0.0\n",
       "17454    0.0\n",
       "2686     1.0\n",
       "5939     1.0\n",
       "43259    1.0\n",
       "3822     1.0\n",
       "45849    1.0\n",
       "34321    0.0\n",
       "7479     1.0\n",
       "35753    1.0\n",
       "44595    1.0\n",
       "34694    0.0\n",
       "19730    0.0\n",
       "11465    1.0\n",
       "8760     1.0\n",
       "35647    0.0\n",
       "36497    1.0\n",
       "13115    0.0\n",
       "20720    0.0\n",
       "16914    0.0\n",
       "Name: class_2, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cl2[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cl2.iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = np.where(y_test_cl2 != clf_2.predict(X_test_cl2_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misclassified[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n"
     ]
    }
   ],
   "source": [
    "for i in range(2185):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "for i in range(len(misclassified[0])):\n",
    "    index = misclassified[0][i]\n",
    "    #print('prediction: {}; correct: {}'.format(pred_prob[index], y_test_cl2.iloc[index]))\n",
    "    #print(0.5 - min(pred_prob[index]))\n",
    "    error.append(0.5 - min(pred_prob[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001853522413891029"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2185,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cl2.iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [0.2823253 0.7176747]\n",
      "actual: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('prediction: {}'.format(pred_prob[25]))\n",
    "print('actual: {}'.format(y_test_cl2.iloc[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORST In writing this review, I would like to note two items of importance.First, I have had the opportunity to read a number of Mr. Ringo's other works and have been generally favorably impressed by their quality.Second, although the information on this book both on Amazon and the cover appears to promote it as a novel in reality it is three stories of significantly different lengths with an attempt to pull them  together with some very uninspired interludes.In summary, I found this book to be two hundred pages of boredom surrounded by several somewhat stories which appear to have been written (and probably rejected) early in the author's career.  I found the long parade of characters to be uninteresting and their motivations to be extremely confused. Would any woman with the experience, intelligence and abilities of the primary character really be the submissive housewife as characterized in this volume - impossible for me to rationalize.It appears to me that either the author or publisher scraped together anything that they could potentially sell and decided to dupe readers of the author's previous works into purchasing this inept mess.Another triumph of commercialism over integrity.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_test_cl2.iloc[25])\n",
    "print(y_test_cl2.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin                                                        0385332947\n",
       "overall                                                              1\n",
       "reviewText           Don't bother to read this one, folks.  Coleman...\n",
       "reviewTime                                                  2006-08-08\n",
       "reviewerID                                              A2OU2NCXK689Z2\n",
       "reviewerName                                          Tigger \"kkegley\"\n",
       "summary                                               Guilty after all\n",
       "unixReviewTime                                              1154995200\n",
       "helpful_votes                                                       12\n",
       "review_age_days                                                   2906\n",
       "annual_HVAR                                                    1.50723\n",
       "book_num_reviews                                                    10\n",
       "std_HVAR                                                      0.988172\n",
       "top_quartile_HVAR                                              1.13868\n",
       "min_max                                      (0.0, 3.2819524727039178)\n",
       "most_helpful                                                         1\n",
       "scaled                                                        0.459247\n",
       "class_2                                                              1\n",
       "class_3                                                              1\n",
       "prepReviewText       WORST Don't bother to read this one, folks.  C...\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratdf_cl2.iloc[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweaking lessons\n",
    "\n",
    "# acc improves with more data\n",
    "# use tfidf max_features and/or regularization parameter to control for overfitting\n",
    "\n",
    "# as per-group samples increase NOT using SVD is better\n",
    "# - might as well experiment with max_features\n",
    "# - 10000 features -> acc decr from 79.8 to 75.4\n",
    "# - 20000 features -> acc 76.6\n",
    "# - 100000 features -> acc 78.2\n",
    "# - 300000 features -> acc 79.4\n",
    "# - 600000 features -> acc 79.9\n",
    "# - 1000000 features -> acc 80.1\n",
    "# - 1.97mil features -> acc 79.8\n",
    "\n",
    "# 32000 examples; 100000 features -> 81.8\n",
    "#               1mil features -> 84.0\n",
    "\n",
    "# do not have vectorizer ignore any tokens (min_df, max_df)\n",
    "# do not remove stop words\n",
    "# increase to (1,3) ngrams\n",
    "# try ngrams (1,3) + truncated svd;\n",
    "# - svd 100 dim acc: .694 (c=0.95)\n",
    "# - svd 120 dim acc: .682 (C=0.95)\n",
    "# - svd 130 dim acc: .685 (C=0.86)\n",
    "# - svd 150 dim acc: .689 (C=0.91)\n",
    "# - svd 50 dim acc: .688 (C=0.91)\n",
    "# - svd 300 dim acc: .689 (C=0.67); more dimensions -> regularization works harder\n",
    "\n",
    "# More data\n",
    "# Did not oversample:\n",
    "#  - sample 1500 per group without replacement\n",
    "#  - svd 300 dim; C=1.0; acc=.732\n",
    "#  - svd 100 dim; C=1.0; acc=.725\n",
    "#  - svd 150 dim; C=0.76; acc=.732\n",
    "\n",
    "# results:\n",
    "\n",
    "#True Negative: 386\n",
    "#True Positive: 492\n",
    "#False Negative: 108\n",
    "#False Positive: 214\n",
    "\n",
    "# Oversample:\n",
    "#  - sample 2000 per group with replacement\n",
    "#  - svd 100 dim; C=0.95; acc=.742\n",
    "#  - svd 150 dim; C=0.81; acc=.745\n",
    "#  - svd 300 dim; C=1.0; acc=.751\n",
    "\n",
    "\n",
    "# oversampling underrepresented increases accuracy\n",
    "\n",
    "# Oversample (8000 total) with no SVD acc: 78.2\n",
    "\n",
    "\n",
    "\n",
    "# reduce dimensions\n",
    "# - stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with truncated SVD\n",
    "dimensions=100\n",
    "svd = TruncatedSVD(n_components=dimensions, n_iter=5, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_prep_vec)\n",
    "X_test_svd = svd.transform(X_test_prep_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 17 epochs took 0 seconds\n",
      "Best value for C: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "clf_3 = LogisticRegression(penalty='l2',\n",
    "                           C=1.0,\n",
    "                           random_state=42,\n",
    "                           solver='saga',\n",
    "                           multi_class='ovr',\n",
    "                           max_iter=100,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True)\n",
    "\n",
    "param_grid = {'C':list(np.linspace(0.1,1.0,20))}            \n",
    "clf_3 = GridSearchCV(clf_3, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=True)\n",
    "clf_3.fit(X_train_svd, y_train_prep)\n",
    "best_c = round(clf_3.best_params_['C'],2)\n",
    "print('Best value for C: {}'.format(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 17 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit a model with the best C value\n",
    "clf_3 = LogisticRegression(penalty='l2',\n",
    "                           C=best_c,\n",
    "                           random_state=42,\n",
    "                           solver='saga',\n",
    "                           multi_class='ovr',\n",
    "                           max_iter=100,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=True)\n",
    "\n",
    "clf_3.fit(X_train_svd, y_train_prep)\n",
    "test_predicted_labels_3 = clf_3.predict(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer - SVD Dimensions = 100\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.735\n",
      "f_1 score (Weighted): 0.735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with various f1 metrics\n",
    "f1_weighted = metrics.f1_score(y_test_prep, test_predicted_labels_3, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test_prep, test_predicted_labels_3)\n",
    "    \n",
    "print('Logistic Regression Classifer - SVD Dimensions = {}'.format(dimensions))\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.3f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.3f}'.format(f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 1186\n",
      "True Positive: 1166\n",
      "False Negative: 434\n",
      "False Positive: 414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test_prep, test_predicted_labels_3)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_prep, test_predicted_labels_3).ravel()\n",
    "print(\"True Negative: {}\".format(tn))\n",
    "print(\"True Positive: {}\".format(tp))\n",
    "print(\"False Negative: {}\".format(fn))\n",
    "print(\"False Positive: {}\".format(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Labeling Strategy\n",
    "Adapt from Jen's labeling notebook\n",
    "1. KBinsDiscretizer with strategy='kmeans'\n",
    "2. Try 3 classes (helpful, unhelpful, undetermined)\n",
    "3. Try training a model on helpful and unhelpful classes only; and on all 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

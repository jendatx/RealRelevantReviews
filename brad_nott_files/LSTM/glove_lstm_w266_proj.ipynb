{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Refs:\n",
    "#    Portions of code borrowed and adapted from the following sources:\n",
    "#        https://www.kaggle.com/snlpnkj/bidirectional-lstm-keras\n",
    "#        https://www.onceupondata.com/2019/02/01/keras-text3-cnn-rnn/\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM, Bidirectional, GlobalMaxPool1D, CuDNNLSTM, concatenate, Flatten\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout, SpatialDropout1D\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Brad\\\\Desktop\\\\Keras - GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "**Notes:**\n",
    "\n",
    "Data was previously grouped and saved to these csv files in `baseline_logreg_cluster.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('w266_proj/data/train_clust_FINAL.csv')\n",
    "dev = pd.read_csv('w266_proj/data/dev_clust_FINAL.csv')\n",
    "test = pd.read_csv('w266_proj/data/test_clust_FINAL.csv')\n",
    "\n",
    "#train_text = train['prepReviewText']\n",
    "#dev_text = dev['prepReviewText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section to test a single data file that has not yet been split into train/dev/test sets\n",
    "# Comment out when those types of files already exist\n",
    "\n",
    "# 2-class labels\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train_cl2, X_test_cl2, y_train_cl2, y_test_cl2 = train_test_split(train.prepReviewText,df_cl2.group_z_class, test_size=0.2, \\\n",
    "                                   #random_state=42,stratify=df_cl2.group_z_class)\n",
    "    \n",
    "#X_train_cl2, X_test_cl2, y_train_cl2, y_test_cl2 = train_test_split(df_cl2.clean_review,df_cl2.class_2, test_size=0.2, \\\n",
    "                                   #random_state=42,stratify=df_cl2.class_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    26066\n",
       "1.0    26066\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of training examples for each class\n",
    "train['group_z_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3008\n",
       "1.0    3008\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of dev examples for each class\n",
    "dev['group_z_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    3066\n",
       "0.0    3066\n",
       "Name: group_z_class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of dev examples for each class\n",
    "test['group_z_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model\n",
    "\n",
    "seed = 101 \n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train = train['prepReviewText']\n",
    "X_dev = dev['prepReviewText']\n",
    "X_test = test['prepReviewText']\n",
    "\n",
    "# Use this line for a multi-class problem to convert class labels\n",
    "#y_train = to_categorical(train['most_helpful'])\n",
    "\n",
    "y_train = train['group_z_class']\n",
    "y_dev = dev['group_z_class']\n",
    "y_test = test['group_z_class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-class problem, use this cell to define class numbers\n",
    "# - Use the variable in the model's final dense/output layer\n",
    "# - Also use softmax activation to get class probabilities for number of classes in num_classes\n",
    "\n",
    "#num_classes = train['group_z_class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52132, 200) (6016, 200) (6132, 200)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text\n",
    "\n",
    "# Tokenize Text\n",
    "max_features = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad examples\n",
    "#max_words = 200\n",
    "max_words = 200\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_dev = sequence.pad_sequences(X_dev, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "print(X_train.shape,X_dev.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('lstm_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "def get_model(max_features, embed_dim, embedding_matrix, learning_rate, spat_drop, lstm_units):\n",
    "    np.random.seed(seed)\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    \n",
    "    # max_features = input_dim (vocab size)\n",
    "    # embed_dim = output_dim (dense embedding size)\n",
    "    model.add(Embedding(max_features,\n",
    "                        embed_dim,\n",
    "                        input_length=X_train.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False))\n",
    "    \n",
    "    model.add(SpatialDropout1D(spat_drop))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences=True)))\n",
    "    \n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer\n",
    "    \n",
    "    adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design embedding matrix\n",
    "# - Use word vectors when they are available\n",
    "# - When they aren't, vector will be random\n",
    "\n",
    "# experiment with various values of max_features\n",
    "\n",
    "def get_embed_mat(EMBEDDING_FILE, max_features=50000):\n",
    "    # word vectors\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    all_embs = np.stack(embeddings_index.values()) #for random init\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n",
    "                                        (num_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    max_features = embedding_matrix.shape[0]\n",
    "    \n",
    "    return max_features, embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#EMBEDDING_FILE = 'w266_proj/glove/glove.6B/glove.6B.100d.txt'\n",
    "#EMBEDDING_FILE = 'w266_proj/glove/glove.6B/glove.6B.200d.txt'\n",
    "#EMBEDDING_FILE = 'w266_proj/glove/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "#EMBEDDING_FILE = 'w266_proj/glove/glove.42B.300d/glove.42B.300d.txt'\n",
    "EMBEDDING_FILE = 'w266_proj/glove/glove.840B.300d/glove.840B.300d.txt'\n",
    "\n",
    "#embed_dim = 100 #word vector dim\n",
    "#embed_dim = 200\n",
    "embed_dim = 300\n",
    "\n",
    "max_features, embedding_matrix = get_embed_mat(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3447: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 450)          948600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 451       \n",
      "=================================================================\n",
      "Total params: 15,949,051\n",
      "Trainable params: 949,051\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From c:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/100\n",
      "52132/52132 [==============================] - 25s 475us/step - loss: 0.5105 - acc: 0.7555 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49319, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "52132/52132 [==============================] - 23s 432us/step - loss: 0.4873 - acc: 0.7728 - val_loss: 0.4868 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49319 to 0.48678, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "52132/52132 [==============================] - 23s 438us/step - loss: 0.4766 - acc: 0.7784 - val_loss: 0.4869 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48678\n",
      "Epoch 4/100\n",
      "52132/52132 [==============================] - 22s 430us/step - loss: 0.4672 - acc: 0.7833 - val_loss: 0.4857 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48678 to 0.48570, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "52132/52132 [==============================] - 23s 433us/step - loss: 0.4571 - acc: 0.7882 - val_loss: 0.4832 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48570 to 0.48324, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "52132/52132 [==============================] - 22s 428us/step - loss: 0.4467 - acc: 0.7944 - val_loss: 0.4812 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48324 to 0.48122, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "52132/52132 [==============================] - 23s 437us/step - loss: 0.4314 - acc: 0.8026 - val_loss: 0.4891 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48122\n",
      "Epoch 8/100\n",
      "52132/52132 [==============================] - 23s 436us/step - loss: 0.4148 - acc: 0.8131 - val_loss: 0.4920 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48122\n",
      "Epoch 9/100\n",
      "52132/52132 [==============================] - 23s 434us/step - loss: 0.3967 - acc: 0.8231 - val_loss: 0.4910 - val_acc: 0.7699\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48122\n",
      "Epoch 10/100\n",
      "52132/52132 [==============================] - 23s 434us/step - loss: 0.3740 - acc: 0.8384 - val_loss: 0.4968 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48122\n",
      "Epoch 11/100\n",
      "52132/52132 [==============================] - 23s 434us/step - loss: 0.3550 - acc: 0.8503 - val_loss: 0.5126 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48122\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232e4de0588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "# Best (max acc):\n",
    "# - 300 dim embeddings\n",
    "# - 200 max words\n",
    "# - 64 batch\n",
    "# - .004 lr\n",
    "# - 175 lstm\n",
    "# - 0.5 spatial dropout\n",
    "\n",
    "# Best (min loss):\n",
    "# - 300 dim embeddings\n",
    "# - 200 max words\n",
    "# - 512 batch\n",
    "# - .001 lr\n",
    "# - 225 lstm\n",
    "# - 0.1 spatial dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Classification problem - early stop on val_acc\n",
    "#es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=5)\n",
    "#mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "# If training acc improves, but val_acc decreases = overfitting training data\n",
    "# Model with highest validation accuracy is the winner\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512 # 64, 128, 256\n",
    "learning_rates = [.1, .01, .004, .003, .001, .0001, .00001] # .001 default\n",
    "\n",
    "model = get_model(max_features, embed_dim, embedding_matrix, learning_rates[4], spat_drop=0.1, lstm_units=225)\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev),epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model: dev set\n",
    "\n",
    "**Note:**\n",
    "\n",
    "This has technically already been done through the parameter tuning process, and the training of the final model. It is repeated here to for ease of visually comparing dev set and test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.96%\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_weights(\"best_model.h5\")\n",
    "\n",
    "scores = model.evaluate(X_dev, y_dev, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model: test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - 2 Class (cluster) Labels FINAL (TEST SET)\n",
      "-------------\n",
      "\n",
      "Accuracy on test set: 0.7684\n",
      "f_1 score (Weighted): 0.7682\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the model on test set\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "test_predicted_labels = model.predict_classes(X_test)\n",
    "f1_weighted = metrics.f1_score(y_test, test_predicted_labels, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test, test_predicted_labels)\n",
    "    \n",
    "print('LSTM - 2 Class (cluster) Labels FINAL (TEST SET)')\n",
    "print('-------------\\n')\n",
    "print('Accuracy on test set: {:0.4f}'.format(accuracy))\n",
    "print('f_1 score (Weighted): {:0.4f}'.format(f1_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model to disk\n",
    "model.save(\"lstm_final_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Error Analysis\n",
    "**Notes:**\n",
    "\n",
    "1. Use this section to store values from the FINAL model run\n",
    "2. Evaluate those results in `LSTM_error_analysis.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store best model's predictions; probability of being helpful (class 1)\n",
    "pred_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions and predicted labels for a dataframe\n",
    "pred_prob_s = pd.Series(pred_prob.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels_s = pd.Series(test_predicted_labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a dataframe\n",
    "frame = {'pred_prob': pred_prob_s, 'pred_class': test_pred_labels_s, 'true_class': y_test}\n",
    "err_analysis = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026451</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.290560</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.775318</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.947763</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.665833</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_prob  pred_class  true_class\n",
       "0   0.026451           0         0.0\n",
       "1   0.290560           0         1.0\n",
       "2   0.775318           1         1.0\n",
       "3   0.947763           1         1.0\n",
       "4   0.665833           1         1.0"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7684279191128506"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that this dataframe produces same accuracy as model run\n",
    "\n",
    "metrics.accuracy_score(err_analysis['true_class'], err_analysis['pred_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off this df for separate error analysis\n",
    "# - error analysis conducted in LSTM_error_analysis.ipynb\n",
    "\n",
    "err_analysis.to_csv('err_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Parameter Tuning\n",
    "**Notes:**\n",
    "\n",
    "1. Results stored in output file `lstm_run_results_val_loss.txt`\n",
    "\n",
    "2. Results viewed/sorted/selected for final model using `lstm_parameter_search_results.ipynb`\n",
    "\n",
    "3. Best results should be used in the **Train Final Model** cell above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.004\n",
      "spatial dropout: 0.5\n",
      "lstm units: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 300)          542400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 15,542,701\n",
      "Trainable params: 542,701\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 14s 266us/step - loss: 0.5131 - acc: 0.7550 - val_loss: 0.5035 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50354, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 14s 259us/step - loss: 0.4905 - acc: 0.7702 - val_loss: 0.4921 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50354 to 0.49206, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 14s 259us/step - loss: 0.4860 - acc: 0.7724 - val_loss: 0.5051 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49206\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 13s 258us/step - loss: 0.4808 - acc: 0.7760 - val_loss: 0.4950 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49206\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 13s 258us/step - loss: 0.4749 - acc: 0.7792 - val_loss: 0.4881 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49206 to 0.48812, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 14s 259us/step - loss: 0.4712 - acc: 0.7803 - val_loss: 0.4841 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48812 to 0.48413, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 13s 258us/step - loss: 0.4666 - acc: 0.7839 - val_loss: 0.5217 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48413\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 13s 259us/step - loss: 0.4608 - acc: 0.7853 - val_loss: 0.4940 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48413\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 14s 260us/step - loss: 0.4586 - acc: 0.7888 - val_loss: 0.4867 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48413\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 13s 258us/step - loss: 0.4549 - acc: 0.7908 - val_loss: 0.4955 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48413\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 14s 259us/step - loss: 0.4462 - acc: 0.7947 - val_loss: 0.5102 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48413\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Training Model 2\n",
      "\n",
      "batch size: 32\n",
      "lr: 0.01\n",
      "spatial dropout: 0.5\n",
      "lstm units: 275\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 550)          1269400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 16,269,951\n",
      "Trainable params: 1,269,951\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 48s 924us/step - loss: 0.5318 - acc: 0.7524 - val_loss: 0.4958 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49581, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 47s 906us/step - loss: 0.5197 - acc: 0.7586 - val_loss: 0.5048 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49581\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 47s 904us/step - loss: 0.5200 - acc: 0.7572 - val_loss: 0.5556 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49581\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 47s 905us/step - loss: 0.5170 - acc: 0.7580 - val_loss: 0.5244 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49581\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 47s 899us/step - loss: 0.5159 - acc: 0.7568 - val_loss: 0.5085 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49581\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 47s 896us/step - loss: 0.5121 - acc: 0.7593 - val_loss: 0.5084 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49581\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Training Model 3\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.005\n",
      "spatial dropout: 0.4\n",
      "lstm units: 100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 200)          321600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 15,321,801\n",
      "Trainable params: 321,801\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 17s 334us/step - loss: 0.5050 - acc: 0.7623 - val_loss: 0.4900 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49005, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 17s 326us/step - loss: 0.4919 - acc: 0.7682 - val_loss: 0.4918 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49005\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 17s 325us/step - loss: 0.4868 - acc: 0.7720 - val_loss: 0.5191 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49005\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 17s 325us/step - loss: 0.4805 - acc: 0.7762 - val_loss: 0.5123 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49005\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 17s 325us/step - loss: 0.4756 - acc: 0.7778 - val_loss: 0.4851 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49005 to 0.48514, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 17s 325us/step - loss: 0.4724 - acc: 0.7798 - val_loss: 0.4933 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48514\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 17s 325us/step - loss: 0.4687 - acc: 0.7817 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48514\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 17s 324us/step - loss: 0.4650 - acc: 0.7834 - val_loss: 0.4973 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48514\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 17s 324us/step - loss: 0.4614 - acc: 0.7851 - val_loss: 0.5061 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48514\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 17s 324us/step - loss: 0.4594 - acc: 0.7862 - val_loss: 0.5050 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48514\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Training Model 4\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.001\n",
      "spatial dropout: 0.3\n",
      "lstm units: 100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 200)          321600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 15,321,801\n",
      "Trainable params: 321,801\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 9s 169us/step - loss: 0.5317 - acc: 0.7350 - val_loss: 0.4991 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49910, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 8s 160us/step - loss: 0.4976 - acc: 0.7662 - val_loss: 0.5000 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49910\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4902 - acc: 0.7713 - val_loss: 0.4915 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49910 to 0.49155, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4873 - acc: 0.7722 - val_loss: 0.4927 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49155\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4834 - acc: 0.7744 - val_loss: 0.4867 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49155 to 0.48666, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4811 - acc: 0.7750 - val_loss: 0.4857 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48666 to 0.48569, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4776 - acc: 0.7764 - val_loss: 0.4871 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48569\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4756 - acc: 0.7776 - val_loss: 0.4838 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48569 to 0.48380, saving model to best_model.h5\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4736 - acc: 0.7802 - val_loss: 0.4850 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48380\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 8s 162us/step - loss: 0.4687 - acc: 0.7823 - val_loss: 0.4826 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48380 to 0.48259, saving model to best_model.h5\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4645 - acc: 0.7839 - val_loss: 0.4815 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48259 to 0.48146, saving model to best_model.h5\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4619 - acc: 0.7851 - val_loss: 0.4833 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48146\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 8s 162us/step - loss: 0.4591 - acc: 0.7869 - val_loss: 0.4877 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48146\n",
      "Epoch 14/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4552 - acc: 0.7900 - val_loss: 0.4888 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48146\n",
      "Epoch 15/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4520 - acc: 0.7906 - val_loss: 0.4838 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48146\n",
      "Epoch 16/20\n",
      "52132/52132 [==============================] - 8s 161us/step - loss: 0.4477 - acc: 0.7929 - val_loss: 0.4860 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48146\n",
      "Epoch 00016: early stopping\n",
      "\n",
      "Training Model 5\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.004\n",
      "spatial dropout: 0.2\n",
      "lstm units: 125\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 250)          427000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 15,427,251\n",
      "Trainable params: 427,251\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 10s 199us/step - loss: 0.5287 - acc: 0.7420 - val_loss: 0.4927 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49275, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 10s 192us/step - loss: 0.4877 - acc: 0.7723 - val_loss: 0.4861 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49275 to 0.48607, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 10s 190us/step - loss: 0.4778 - acc: 0.7768 - val_loss: 0.4875 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48607\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 10s 190us/step - loss: 0.4700 - acc: 0.7815 - val_loss: 0.4872 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48607\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4609 - acc: 0.7861 - val_loss: 0.5013 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48607\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4499 - acc: 0.7922 - val_loss: 0.4856 - val_acc: 0.7773\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48607 to 0.48561, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4378 - acc: 0.7979 - val_loss: 0.4866 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48561\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4249 - acc: 0.8069 - val_loss: 0.4893 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48561\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4058 - acc: 0.8172 - val_loss: 0.4867 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48561\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.3904 - acc: 0.8269 - val_loss: 0.5056 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48561\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.3731 - acc: 0.8363 - val_loss: 0.5067 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48561\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Training Model 6\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.01\n",
      "spatial dropout: 0.3\n",
      "lstm units: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 300)          542400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 15,542,701\n",
      "Trainable params: 542,701\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 14s 271us/step - loss: 0.5262 - acc: 0.7549 - val_loss: 0.4998 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49976, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 14s 262us/step - loss: 0.4849 - acc: 0.7719 - val_loss: 0.4860 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49976 to 0.48599, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 14s 261us/step - loss: 0.4786 - acc: 0.7770 - val_loss: 0.4876 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48599\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 14s 261us/step - loss: 0.4685 - acc: 0.7814 - val_loss: 0.4900 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48599\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 14s 261us/step - loss: 0.4598 - acc: 0.7858 - val_loss: 0.4845 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48599 to 0.48445, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 14s 260us/step - loss: 0.4517 - acc: 0.7912 - val_loss: 0.5570 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48445\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 14s 260us/step - loss: 0.4430 - acc: 0.7960 - val_loss: 0.4920 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48445\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 14s 260us/step - loss: 0.4273 - acc: 0.8036 - val_loss: 0.4980 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48445\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 14s 260us/step - loss: 0.4199 - acc: 0.8083 - val_loss: 0.5249 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48445\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 14s 259us/step - loss: 0.4047 - acc: 0.8173 - val_loss: 0.5125 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48445\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Training Model 7\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.002\n",
      "spatial dropout: 0.4\n",
      "lstm units: 100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 200)          321600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 15,321,801\n",
      "Trainable params: 321,801\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 10s 183us/step - loss: 0.5129 - acc: 0.7539 - val_loss: 0.4938 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49376, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4904 - acc: 0.7704 - val_loss: 0.4881 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49376 to 0.48811, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 9s 174us/step - loss: 0.4860 - acc: 0.7738 - val_loss: 0.4878 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48811 to 0.48775, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 9s 175us/step - loss: 0.4790 - acc: 0.7761 - val_loss: 0.5033 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48775\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4758 - acc: 0.7788 - val_loss: 0.4846 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48775 to 0.48463, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4699 - acc: 0.7814 - val_loss: 0.4995 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48463\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4646 - acc: 0.7839 - val_loss: 0.4907 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48463\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 9s 172us/step - loss: 0.4607 - acc: 0.7859 - val_loss: 0.4827 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48463 to 0.48271, saving model to best_model.h5\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4545 - acc: 0.7908 - val_loss: 0.4820 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48271 to 0.48198, saving model to best_model.h5\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4515 - acc: 0.7909 - val_loss: 0.5023 - val_acc: 0.7598\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48198\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 9s 172us/step - loss: 0.4425 - acc: 0.7980 - val_loss: 0.4873 - val_acc: 0.7773\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48198\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 9s 173us/step - loss: 0.4371 - acc: 0.8004 - val_loss: 0.4873 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48198\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 9s 171us/step - loss: 0.4311 - acc: 0.8020 - val_loss: 0.4891 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48198\n",
      "Epoch 14/20\n",
      "52132/52132 [==============================] - 9s 172us/step - loss: 0.4259 - acc: 0.8069 - val_loss: 0.4873 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48198\n",
      "Epoch 00014: early stopping\n",
      "\n",
      "Training Model 8\n",
      "\n",
      "batch size: 32\n",
      "lr: 0.004\n",
      "spatial dropout: 0.3\n",
      "lstm units: 325\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 650)          1630200   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 650)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 651       \n",
      "=================================================================\n",
      "Total params: 16,630,851\n",
      "Trainable params: 1,630,851\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 59s 1ms/step - loss: 0.5098 - acc: 0.7616 - val_loss: 0.4911 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49105, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4930 - acc: 0.7693 - val_loss: 0.4894 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49105 to 0.48939, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4855 - acc: 0.7726 - val_loss: 0.4907 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48939\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4751 - acc: 0.7797 - val_loss: 0.4884 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48939 to 0.48843, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4645 - acc: 0.7830 - val_loss: 0.4966 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48843\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4553 - acc: 0.7887 - val_loss: 0.4998 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48843\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4424 - acc: 0.7943 - val_loss: 0.5053 - val_acc: 0.7671\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48843\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4335 - acc: 0.8010 - val_loss: 0.5099 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48843\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 58s 1ms/step - loss: 0.4205 - acc: 0.8069 - val_loss: 0.5459 - val_acc: 0.7443\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48843\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 9\n",
      "\n",
      "batch size: 128\n",
      "lr: 0.001\n",
      "spatial dropout: 0.2\n",
      "lstm units: 300\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 600)          1444800   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 601       \n",
      "=================================================================\n",
      "Total params: 16,445,401\n",
      "Trainable params: 1,445,401\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 37s 705us/step - loss: 0.5009 - acc: 0.7630 - val_loss: 0.4984 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49843, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 31s 602us/step - loss: 0.4806 - acc: 0.7741 - val_loss: 0.5241 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49843\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 32s 608us/step - loss: 0.4670 - acc: 0.7832 - val_loss: 0.4916 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49843 to 0.49158, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 32s 606us/step - loss: 0.4507 - acc: 0.7915 - val_loss: 0.4904 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49158 to 0.49038, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 32s 609us/step - loss: 0.4301 - acc: 0.8039 - val_loss: 0.4870 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49038 to 0.48701, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 32s 606us/step - loss: 0.4021 - acc: 0.8212 - val_loss: 0.4870 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48701\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 32s 606us/step - loss: 0.3696 - acc: 0.8395 - val_loss: 0.4997 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48701\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 32s 608us/step - loss: 0.3311 - acc: 0.8649 - val_loss: 0.5134 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48701\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 32s 611us/step - loss: 0.2901 - acc: 0.8912 - val_loss: 0.5266 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48701\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 32s 608us/step - loss: 0.2497 - acc: 0.9127 - val_loss: 0.5505 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48701\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Training Model 10\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.001\n",
      "spatial dropout: 0.5\n",
      "lstm units: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 300)          542400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 15,542,701\n",
      "Trainable params: 542,701\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.5047 - acc: 0.7610 - val_loss: 0.4938 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49376, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 21s 411us/step - loss: 0.4919 - acc: 0.7690 - val_loss: 0.5256 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49376\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 21s 409us/step - loss: 0.4860 - acc: 0.7734 - val_loss: 0.4919 - val_acc: 0.7738\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49376 to 0.49195, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 21s 409us/step - loss: 0.4805 - acc: 0.7760 - val_loss: 0.4871 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49195 to 0.48715, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4762 - acc: 0.7782 - val_loss: 0.4878 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48715\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4719 - acc: 0.7798 - val_loss: 0.5247 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48715\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4659 - acc: 0.7829 - val_loss: 0.4931 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48715\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4592 - acc: 0.7866 - val_loss: 0.4839 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48715 to 0.48388, saving model to best_model.h5\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4534 - acc: 0.7899 - val_loss: 0.4981 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48388\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4470 - acc: 0.7937 - val_loss: 0.4907 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48388\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 21s 408us/step - loss: 0.4441 - acc: 0.7953 - val_loss: 0.4923 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48388\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 21s 409us/step - loss: 0.4360 - acc: 0.8001 - val_loss: 0.4910 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48388\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 21s 410us/step - loss: 0.4291 - acc: 0.8045 - val_loss: 0.5061 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48388\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Training Model 11\n",
      "\n",
      "batch size: 128\n",
      "lr: 0.004\n",
      "spatial dropout: 0.2\n",
      "lstm units: 75\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 150)          226200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 15,226,351\n",
      "Trainable params: 226,351\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 11s 202us/step - loss: 0.5006 - acc: 0.7635 - val_loss: 0.4861 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48605, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.4826 - acc: 0.7751 - val_loss: 0.4981 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48605\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.4711 - acc: 0.7798 - val_loss: 0.4850 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48605 to 0.48495, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 10s 194us/step - loss: 0.4589 - acc: 0.7874 - val_loss: 0.4823 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48495 to 0.48235, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.4464 - acc: 0.7942 - val_loss: 0.4933 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48235\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.4321 - acc: 0.8011 - val_loss: 0.4964 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48235\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 10s 192us/step - loss: 0.4111 - acc: 0.8146 - val_loss: 0.5123 - val_acc: 0.7743\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48235\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.3936 - acc: 0.8243 - val_loss: 0.5156 - val_acc: 0.7603\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48235\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.3794 - acc: 0.8316 - val_loss: 0.5249 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48235\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 12\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.01\n",
      "spatial dropout: 0.4\n",
      "lstm units: 275\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 550)          1269400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 16,269,951\n",
      "Trainable params: 1,269,951\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 24s 463us/step - loss: 0.5794 - acc: 0.7460 - val_loss: 0.5056 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50557, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 23s 450us/step - loss: 0.5039 - acc: 0.7635 - val_loss: 0.4925 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50557 to 0.49247, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 24s 451us/step - loss: 0.4917 - acc: 0.7690 - val_loss: 0.5414 - val_acc: 0.7399\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49247\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 23s 451us/step - loss: 0.4867 - acc: 0.7709 - val_loss: 0.4974 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49247\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 24s 451us/step - loss: 0.4831 - acc: 0.7739 - val_loss: 0.4854 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49247 to 0.48535, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 23s 451us/step - loss: 0.4704 - acc: 0.7803 - val_loss: 0.5060 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48535\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 24s 452us/step - loss: 0.4626 - acc: 0.7845 - val_loss: 0.4842 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48535 to 0.48415, saving model to best_model.h5\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 24s 451us/step - loss: 0.4594 - acc: 0.7850 - val_loss: 0.4861 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48415\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 24s 451us/step - loss: 0.4511 - acc: 0.7896 - val_loss: 0.5526 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48415\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 24s 451us/step - loss: 0.4429 - acc: 0.7946 - val_loss: 0.4996 - val_acc: 0.7701\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48415\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 23s 450us/step - loss: 0.4358 - acc: 0.7973 - val_loss: 0.4994 - val_acc: 0.7701\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48415\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 24s 452us/step - loss: 0.4290 - acc: 0.8020 - val_loss: 0.5118 - val_acc: 0.7603\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48415\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Training Model 13\n",
      "\n",
      "batch size: 32\n",
      "lr: 0.01\n",
      "spatial dropout: 0.5\n",
      "lstm units: 275\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 550)          1269400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 16,269,951\n",
      "Trainable params: 1,269,951\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 47s 908us/step - loss: 0.5318 - acc: 0.7524 - val_loss: 0.4958 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49581, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 47s 897us/step - loss: 0.5197 - acc: 0.7586 - val_loss: 0.5048 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49581\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 46s 883us/step - loss: 0.5200 - acc: 0.7572 - val_loss: 0.5556 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49581\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 46s 883us/step - loss: 0.5170 - acc: 0.7580 - val_loss: 0.5244 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49581\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 46s 884us/step - loss: 0.5159 - acc: 0.7568 - val_loss: 0.5085 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49581\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 46s 883us/step - loss: 0.5121 - acc: 0.7593 - val_loss: 0.5084 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49581\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "Training Model 14\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.001\n",
      "spatial dropout: 0.1\n",
      "lstm units: 225\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 450)          948600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 451       \n",
      "=================================================================\n",
      "Total params: 15,949,051\n",
      "Trainable params: 949,051\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 24s 452us/step - loss: 0.5105 - acc: 0.7555 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49319, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 23s 443us/step - loss: 0.4873 - acc: 0.7728 - val_loss: 0.4868 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49319 to 0.48678, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 23s 445us/step - loss: 0.4766 - acc: 0.7784 - val_loss: 0.4869 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48678\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.4672 - acc: 0.7833 - val_loss: 0.4857 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48678 to 0.48570, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.4571 - acc: 0.7882 - val_loss: 0.4832 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48570 to 0.48324, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 23s 443us/step - loss: 0.4467 - acc: 0.7944 - val_loss: 0.4812 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48324 to 0.48122, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.4314 - acc: 0.8026 - val_loss: 0.4891 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48122\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 23s 443us/step - loss: 0.4148 - acc: 0.8131 - val_loss: 0.4920 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48122\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.3967 - acc: 0.8231 - val_loss: 0.4910 - val_acc: 0.7699\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48122\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.3740 - acc: 0.8384 - val_loss: 0.4968 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48122\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 23s 444us/step - loss: 0.3550 - acc: 0.8503 - val_loss: 0.5126 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48122\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Training Model 15\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.004\n",
      "spatial dropout: 0.1\n",
      "lstm units: 275\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 550)          1269400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 16,269,951\n",
      "Trainable params: 1,269,951\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 24s 466us/step - loss: 0.5336 - acc: 0.7480 - val_loss: 0.4900 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48999, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.4826 - acc: 0.7739 - val_loss: 0.4932 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48999\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.4704 - acc: 0.7802 - val_loss: 0.4834 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48999 to 0.48343, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.4601 - acc: 0.7868 - val_loss: 0.5008 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48343\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 24s 455us/step - loss: 0.4386 - acc: 0.7985 - val_loss: 0.4822 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48343 to 0.48223, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.4205 - acc: 0.8079 - val_loss: 0.4898 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48223\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 24s 455us/step - loss: 0.3880 - acc: 0.8304 - val_loss: 0.4913 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48223\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.3558 - acc: 0.8497 - val_loss: 0.4975 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48223\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.3167 - acc: 0.8733 - val_loss: 0.5431 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48223\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 24s 454us/step - loss: 0.2695 - acc: 0.9056 - val_loss: 0.5325 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48223\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Training Model 16\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.01\n",
      "spatial dropout: 0.5\n",
      "lstm units: 300\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 600)          1444800   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 601       \n",
      "=================================================================\n",
      "Total params: 16,445,401\n",
      "Trainable params: 1,445,401\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 28s 539us/step - loss: 0.5933 - acc: 0.7446 - val_loss: 0.5078 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50776, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 27s 527us/step - loss: 0.5022 - acc: 0.7644 - val_loss: 0.5559 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50776\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 27s 527us/step - loss: 0.5035 - acc: 0.7638 - val_loss: 0.5432 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50776\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 27s 527us/step - loss: 0.5055 - acc: 0.7613 - val_loss: 0.4992 - val_acc: 0.7691\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50776 to 0.49924, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 27s 526us/step - loss: 0.5014 - acc: 0.7643 - val_loss: 0.5340 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49924\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4965 - acc: 0.7684 - val_loss: 0.4959 - val_acc: 0.7704\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49924 to 0.49590, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4945 - acc: 0.7680 - val_loss: 0.5439 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49590\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 27s 526us/step - loss: 0.4924 - acc: 0.7689 - val_loss: 0.5248 - val_acc: 0.7515\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49590\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4935 - acc: 0.7682 - val_loss: 0.4989 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49590\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4940 - acc: 0.7683 - val_loss: 0.4934 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49590 to 0.49345, saving model to best_model.h5\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 27s 524us/step - loss: 0.4942 - acc: 0.7668 - val_loss: 0.5173 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49345\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4912 - acc: 0.7699 - val_loss: 0.5261 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49345\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 27s 524us/step - loss: 0.4904 - acc: 0.7705 - val_loss: 0.5072 - val_acc: 0.7631\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49345\n",
      "Epoch 14/20\n",
      "52132/52132 [==============================] - 27s 525us/step - loss: 0.4870 - acc: 0.7706 - val_loss: 0.5229 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49345\n",
      "Epoch 15/20\n",
      "52132/52132 [==============================] - 27s 526us/step - loss: 0.4793 - acc: 0.7757 - val_loss: 0.5177 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49345\n",
      "Epoch 00015: early stopping\n",
      "\n",
      "Training Model 17\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.01\n",
      "spatial dropout: 0.1\n",
      "lstm units: 225\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 450)          948600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 451       \n",
      "=================================================================\n",
      "Total params: 15,949,051\n",
      "Trainable params: 949,051\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 22s 430us/step - loss: 0.6219 - acc: 0.7283 - val_loss: 0.4982 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49824, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4969 - acc: 0.7672 - val_loss: 0.4913 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49824 to 0.49134, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4856 - acc: 0.7722 - val_loss: 0.4858 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49134 to 0.48577, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4740 - acc: 0.7783 - val_loss: 0.4867 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48577\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 22s 420us/step - loss: 0.4646 - acc: 0.7825 - val_loss: 0.4849 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48577 to 0.48488, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4550 - acc: 0.7877 - val_loss: 0.4851 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48488\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4325 - acc: 0.8012 - val_loss: 0.4978 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48488\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.4065 - acc: 0.8164 - val_loss: 0.5021 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48488\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 22s 421us/step - loss: 0.3853 - acc: 0.8309 - val_loss: 0.5176 - val_acc: 0.7595\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48488\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 22s 422us/step - loss: 0.3497 - acc: 0.8505 - val_loss: 0.5355 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48488\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Training Model 18\n",
      "\n",
      "batch size: 256\n",
      "lr: 0.003\n",
      "spatial dropout: 0.5\n",
      "lstm units: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 300)          542400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 15,542,701\n",
      "Trainable params: 542,701\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 15s 294us/step - loss: 0.5087 - acc: 0.7564 - val_loss: 0.4895 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48953, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 15s 284us/step - loss: 0.4927 - acc: 0.7690 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48953\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 15s 285us/step - loss: 0.4852 - acc: 0.7738 - val_loss: 0.4946 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48953\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 15s 285us/step - loss: 0.4791 - acc: 0.7770 - val_loss: 0.4848 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48953 to 0.48478, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 15s 286us/step - loss: 0.4747 - acc: 0.7781 - val_loss: 0.4983 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48478\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 15s 285us/step - loss: 0.4696 - acc: 0.7816 - val_loss: 0.5205 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48478\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 15s 286us/step - loss: 0.4661 - acc: 0.7838 - val_loss: 0.5064 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48478\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 15s 285us/step - loss: 0.4593 - acc: 0.7866 - val_loss: 0.4980 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48478\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 15s 285us/step - loss: 0.4567 - acc: 0.7894 - val_loss: 0.5035 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48478\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 19\n",
      "\n",
      "batch size: 128\n",
      "lr: 0.001\n",
      "spatial dropout: 0.2\n",
      "lstm units: 200\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 400)          803200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 15,803,601\n",
      "Trainable params: 803,601\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 21s 406us/step - loss: 0.5003 - acc: 0.7642 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48997, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.4823 - acc: 0.7743 - val_loss: 0.4860 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48997 to 0.48605, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.4699 - acc: 0.7807 - val_loss: 0.4848 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48605 to 0.48479, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.4579 - acc: 0.7884 - val_loss: 0.4820 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48479 to 0.48201, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.4393 - acc: 0.7982 - val_loss: 0.4861 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48201\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.4197 - acc: 0.8089 - val_loss: 0.4878 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48201\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 21s 394us/step - loss: 0.3968 - acc: 0.8240 - val_loss: 0.4900 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48201\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 21s 395us/step - loss: 0.3672 - acc: 0.8417 - val_loss: 0.4964 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48201\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 21s 394us/step - loss: 0.3366 - acc: 0.8611 - val_loss: 0.5101 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48201\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 20\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.004\n",
      "spatial dropout: 0.1\n",
      "lstm units: 75\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 150)          226200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 15,226,351\n",
      "Trainable params: 226,351\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 16s 303us/step - loss: 0.4993 - acc: 0.7658 - val_loss: 0.4872 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48724, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 15s 294us/step - loss: 0.4804 - acc: 0.7757 - val_loss: 0.4867 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48724 to 0.48674, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 15s 294us/step - loss: 0.4655 - acc: 0.7831 - val_loss: 0.4901 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48674\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 15s 292us/step - loss: 0.4427 - acc: 0.7958 - val_loss: 0.4937 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48674\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 15s 292us/step - loss: 0.4207 - acc: 0.8090 - val_loss: 0.5065 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48674\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 15s 293us/step - loss: 0.3902 - acc: 0.8259 - val_loss: 0.5486 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48674\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 15s 292us/step - loss: 0.3591 - acc: 0.8451 - val_loss: 0.5510 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48674\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Training Model 21\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.003\n",
      "spatial dropout: 0.5\n",
      "lstm units: 400\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 800)          2246400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 17,247,201\n",
      "Trainable params: 2,247,201\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.5084 - acc: 0.7622 - val_loss: 0.5340 - val_acc: 0.7402\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53398, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4946 - acc: 0.7691 - val_loss: 0.4981 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53398 to 0.49806, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4943 - acc: 0.7693 - val_loss: 0.5037 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49806\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4859 - acc: 0.7723 - val_loss: 0.4984 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49806\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4796 - acc: 0.7785 - val_loss: 0.4958 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49806 to 0.49580, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4739 - acc: 0.7790 - val_loss: 0.4903 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49580 to 0.49033, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4713 - acc: 0.7808 - val_loss: 0.4886 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49033 to 0.48865, saving model to best_model.h5\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4663 - acc: 0.7843 - val_loss: 0.4956 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48865\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4613 - acc: 0.7869 - val_loss: 0.4931 - val_acc: 0.7738\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48865\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4602 - acc: 0.7871 - val_loss: 0.5120 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48865\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4527 - acc: 0.7896 - val_loss: 0.5066 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48865\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 54s 1ms/step - loss: 0.4479 - acc: 0.7925 - val_loss: 0.5296 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48865\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Training Model 22\n",
      "\n",
      "batch size: 128\n",
      "lr: 0.003\n",
      "spatial dropout: 0.5\n",
      "lstm units: 75\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 150)          226200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 15,226,351\n",
      "Trainable params: 226,351\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 10s 201us/step - loss: 0.5060 - acc: 0.7600 - val_loss: 0.4991 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49911, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 10s 192us/step - loss: 0.4920 - acc: 0.7688 - val_loss: 0.5086 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49911\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4873 - acc: 0.7707 - val_loss: 0.4988 - val_acc: 0.7658\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49911 to 0.49875, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4811 - acc: 0.7756 - val_loss: 0.4851 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49875 to 0.48507, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4802 - acc: 0.7754 - val_loss: 0.5224 - val_acc: 0.7443\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48507\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4755 - acc: 0.7786 - val_loss: 0.5147 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48507\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4726 - acc: 0.7801 - val_loss: 0.4874 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48507\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4686 - acc: 0.7834 - val_loss: 0.4912 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48507\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 10s 191us/step - loss: 0.4683 - acc: 0.7825 - val_loss: 0.5041 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48507\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 23\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.005\n",
      "spatial dropout: 0.5\n",
      "lstm units: 325\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 650)          1630200   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 650)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 651       \n",
      "=================================================================\n",
      "Total params: 16,630,851\n",
      "Trainable params: 1,630,851\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 29s 547us/step - loss: 0.5888 - acc: 0.7354 - val_loss: 0.5001 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50011, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4939 - acc: 0.7687 - val_loss: 0.4955 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50011 to 0.49550, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 28s 537us/step - loss: 0.4871 - acc: 0.7731 - val_loss: 0.4848 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49550 to 0.48484, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 28s 535us/step - loss: 0.4842 - acc: 0.7732 - val_loss: 0.4918 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48484\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4775 - acc: 0.7780 - val_loss: 0.4991 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48484\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4725 - acc: 0.7797 - val_loss: 0.5142 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48484\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4688 - acc: 0.7827 - val_loss: 0.4828 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48484 to 0.48278, saving model to best_model.h5\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4638 - acc: 0.7846 - val_loss: 0.4843 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48278\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4557 - acc: 0.7895 - val_loss: 0.5025 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48278\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4507 - acc: 0.7916 - val_loss: 0.5102 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48278\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52132/52132 [==============================] - 28s 536us/step - loss: 0.4499 - acc: 0.7928 - val_loss: 0.4878 - val_acc: 0.7811\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48278\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 28s 537us/step - loss: 0.4372 - acc: 0.7986 - val_loss: 0.4874 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48278\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "Training Model 24\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.01\n",
      "spatial dropout: 0.4\n",
      "lstm units: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 300)          542400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 15,542,701\n",
      "Trainable params: 542,701\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 22s 415us/step - loss: 0.5129 - acc: 0.7574 - val_loss: 0.5006 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50062, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 21s 405us/step - loss: 0.5020 - acc: 0.7636 - val_loss: 0.5123 - val_acc: 0.7540\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50062\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 21s 403us/step - loss: 0.4961 - acc: 0.7671 - val_loss: 0.5012 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50062\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4919 - acc: 0.7687 - val_loss: 0.5135 - val_acc: 0.7558\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50062\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4934 - acc: 0.7683 - val_loss: 0.4947 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50062 to 0.49467, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4893 - acc: 0.7712 - val_loss: 0.5397 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49467\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 21s 405us/step - loss: 0.4868 - acc: 0.7728 - val_loss: 0.5147 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49467\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4834 - acc: 0.7752 - val_loss: 0.4928 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49467 to 0.49283, saving model to best_model.h5\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4852 - acc: 0.7732 - val_loss: 0.4950 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49283\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 21s 403us/step - loss: 0.4812 - acc: 0.7748 - val_loss: 0.5267 - val_acc: 0.7478\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49283\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 21s 404us/step - loss: 0.4816 - acc: 0.7719 - val_loss: 0.4986 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49283\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 21s 403us/step - loss: 0.4812 - acc: 0.7732 - val_loss: 0.5001 - val_acc: 0.7671\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49283\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 21s 405us/step - loss: 0.4778 - acc: 0.7752 - val_loss: 0.5032 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49283\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Training Model 25\n",
      "\n",
      "batch size: 256\n",
      "lr: 0.002\n",
      "spatial dropout: 0.1\n",
      "lstm units: 100\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 200)          321600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 15,321,801\n",
      "Trainable params: 321,801\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 10s 193us/step - loss: 0.5025 - acc: 0.7607 - val_loss: 0.4885 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48855, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.4833 - acc: 0.7738 - val_loss: 0.4836 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48855 to 0.48355, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.4691 - acc: 0.7817 - val_loss: 0.4814 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48355 to 0.48137, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.4541 - acc: 0.7912 - val_loss: 0.5002 - val_acc: 0.7621\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48137\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.4369 - acc: 0.7993 - val_loss: 0.4848 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48137\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.4179 - acc: 0.8110 - val_loss: 0.4905 - val_acc: 0.7779\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48137\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.3916 - acc: 0.8272 - val_loss: 0.5026 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48137\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 10s 185us/step - loss: 0.3646 - acc: 0.8440 - val_loss: 0.5111 - val_acc: 0.7621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48137\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "Training Model 26\n",
      "\n",
      "batch size: 32\n",
      "lr: 0.002\n",
      "spatial dropout: 0.2\n",
      "lstm units: 225\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 450)          948600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 451       \n",
      "=================================================================\n",
      "Total params: 15,949,051\n",
      "Trainable params: 949,051\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 41s 794us/step - loss: 0.5008 - acc: 0.7658 - val_loss: 0.4896 - val_acc: 0.7773\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48959, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 41s 779us/step - loss: 0.4829 - acc: 0.7736 - val_loss: 0.4860 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48959 to 0.48602, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 41s 779us/step - loss: 0.4685 - acc: 0.7831 - val_loss: 0.4836 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48602 to 0.48356, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 41s 792us/step - loss: 0.4482 - acc: 0.7925 - val_loss: 0.4913 - val_acc: 0.7768\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48356\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 41s 782us/step - loss: 0.4206 - acc: 0.8076 - val_loss: 0.5002 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48356\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 41s 782us/step - loss: 0.3849 - acc: 0.8284 - val_loss: 0.5157 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48356\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 41s 787us/step - loss: 0.3455 - acc: 0.8521 - val_loss: 0.5452 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48356\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 41s 787us/step - loss: 0.3057 - acc: 0.8731 - val_loss: 0.5777 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48356\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "Training Model 27\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.001\n",
      "spatial dropout: 0.5\n",
      "lstm units: 350\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 700)          1825600   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 16,826,301\n",
      "Trainable params: 1,826,301\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 31s 585us/step - loss: 0.5167 - acc: 0.7521 - val_loss: 0.5219 - val_acc: 0.7429\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52189, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 30s 571us/step - loss: 0.4944 - acc: 0.7679 - val_loss: 0.4971 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52189 to 0.49707, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4926 - acc: 0.7688 - val_loss: 0.4977 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49707\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4867 - acc: 0.7731 - val_loss: 0.5057 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49707\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4839 - acc: 0.7747 - val_loss: 0.5007 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49707\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4796 - acc: 0.7763 - val_loss: 0.4929 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49707 to 0.49294, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4758 - acc: 0.7786 - val_loss: 0.4938 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49294\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 30s 571us/step - loss: 0.4739 - acc: 0.7799 - val_loss: 0.5230 - val_acc: 0.7497\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49294\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4704 - acc: 0.7804 - val_loss: 0.4923 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.49294 to 0.49229, saving model to best_model.h5\n",
      "Epoch 10/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4642 - acc: 0.7840 - val_loss: 0.4903 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49229 to 0.49028, saving model to best_model.h5\n",
      "Epoch 11/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4626 - acc: 0.7847 - val_loss: 0.5049 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49028\n",
      "Epoch 12/20\n",
      "52132/52132 [==============================] - 30s 573us/step - loss: 0.4572 - acc: 0.7884 - val_loss: 0.5066 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49028\n",
      "Epoch 13/20\n",
      "52132/52132 [==============================] - 30s 571us/step - loss: 0.4535 - acc: 0.7898 - val_loss: 0.4949 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49028\n",
      "Epoch 14/20\n",
      "52132/52132 [==============================] - 30s 573us/step - loss: 0.4482 - acc: 0.7937 - val_loss: 0.4995 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49028\n",
      "Epoch 15/20\n",
      "52132/52132 [==============================] - 30s 572us/step - loss: 0.4468 - acc: 0.7937 - val_loss: 0.4886 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.49028 to 0.48855, saving model to best_model.h5\n",
      "Epoch 16/20\n",
      "52132/52132 [==============================] - 30s 572us/step - loss: 0.4384 - acc: 0.7985 - val_loss: 0.4945 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48855\n",
      "Epoch 17/20\n",
      "52132/52132 [==============================] - 30s 571us/step - loss: 0.4325 - acc: 0.8020 - val_loss: 0.4917 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48855\n",
      "Epoch 18/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4288 - acc: 0.8056 - val_loss: 0.5115 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48855\n",
      "Epoch 19/20\n",
      "52132/52132 [==============================] - 30s 570us/step - loss: 0.4194 - acc: 0.8115 - val_loss: 0.5365 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48855\n",
      "Epoch 20/20\n",
      "52132/52132 [==============================] - 30s 571us/step - loss: 0.4130 - acc: 0.8139 - val_loss: 0.5034 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48855\n",
      "Epoch 00020: early stopping\n",
      "\n",
      "Training Model 28\n",
      "\n",
      "batch size: 64\n",
      "lr: 0.005\n",
      "spatial dropout: 0.4\n",
      "lstm units: 125\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 250)          427000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 15,427,251\n",
      "Trainable params: 427,251\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 19s 369us/step - loss: 0.5075 - acc: 0.7594 - val_loss: 0.4879 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48787, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 19s 358us/step - loss: 0.4917 - acc: 0.7689 - val_loss: 0.4872 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48787 to 0.48720, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 19s 358us/step - loss: 0.4885 - acc: 0.7706 - val_loss: 0.5050 - val_acc: 0.7616\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48720\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 19s 356us/step - loss: 0.4816 - acc: 0.7746 - val_loss: 0.4914 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48720\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 19s 357us/step - loss: 0.4779 - acc: 0.7781 - val_loss: 0.5218 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48720\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 19s 357us/step - loss: 0.4750 - acc: 0.7779 - val_loss: 0.4886 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48720\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 19s 357us/step - loss: 0.4680 - acc: 0.7815 - val_loss: 0.5001 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48720\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Training Model 29\n",
      "\n",
      "batch size: 512\n",
      "lr: 0.004\n",
      "spatial dropout: 0.1\n",
      "lstm units: 275\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 550)          1269400   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 16,269,951\n",
      "Trainable params: 1,269,951\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n",
      "52132/52132 [==============================] - 26s 496us/step - loss: 0.5123 - acc: 0.7622 - val_loss: 0.4902 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49016, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "52132/52132 [==============================] - 25s 484us/step - loss: 0.4791 - acc: 0.7750 - val_loss: 0.4887 - val_acc: 0.7733\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49016 to 0.48874, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "52132/52132 [==============================] - 25s 485us/step - loss: 0.4630 - acc: 0.7846 - val_loss: 0.5622 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48874\n",
      "Epoch 4/20\n",
      "52132/52132 [==============================] - 25s 486us/step - loss: 0.4411 - acc: 0.7968 - val_loss: 0.4839 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48874 to 0.48394, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "52132/52132 [==============================] - 25s 486us/step - loss: 0.4098 - acc: 0.8143 - val_loss: 0.4958 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48394\n",
      "Epoch 6/20\n",
      "52132/52132 [==============================] - 25s 485us/step - loss: 0.3735 - acc: 0.8364 - val_loss: 0.4942 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48394\n",
      "Epoch 7/20\n",
      "52132/52132 [==============================] - 25s 484us/step - loss: 0.3216 - acc: 0.8720 - val_loss: 0.5059 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48394\n",
      "Epoch 8/20\n",
      "52132/52132 [==============================] - 25s 485us/step - loss: 0.2620 - acc: 0.9071 - val_loss: 0.5303 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48394\n",
      "Epoch 9/20\n",
      "52132/52132 [==============================] - 25s 484us/step - loss: 0.2075 - acc: 0.9365 - val_loss: 0.5698 - val_acc: 0.7581\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48394\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Training Model 30\n",
      "\n",
      "batch size: 1024\n",
      "lr: 0.001\n",
      "spatial dropout: 0.1\n",
      "lstm units: 375\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 750)          2031000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 751       \n",
      "=================================================================\n",
      "Total params: 17,031,751\n",
      "Trainable params: 2,031,751\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 52132 samples, validate on 6016 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,200,750] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_1/concat_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7b5d01008dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Build and fit model for run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# load weights for best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2715\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2717\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2718\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2719\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2677\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brad\\desktop\\keras-~1\\test\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,200,750] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_1/concat_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Random parameter search \n",
    "\n",
    "# given 200 words; 300 dim\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(1,100):\n",
    "    \n",
    "    print()\n",
    "    print('Training Model {}'.format(i))\n",
    "    print()\n",
    "    \n",
    "    #es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=5)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    #mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "    \n",
    "    # Define parameters for run\n",
    "    epochs = 20\n",
    "    #batch_size = random.randint(32,513)\n",
    "    batch_size = random.choice([32, 64, 128, 256, 512, 1024])\n",
    "    lr = random.choice([.001, .002, .003, .004, .005, .01])\n",
    "    dropout = random.choice([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "    #lstm_units = random.randint(50,401)\n",
    "    lstm_units = random.choice([75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400])\n",
    "    \n",
    "    print('batch size: {}'.format(batch_size))\n",
    "    print('lr: {}'.format(lr))\n",
    "    print('spatial dropout: {}'.format(dropout))\n",
    "    print('lstm units: {}'.format(lstm_units))\n",
    "    \n",
    "    # Build and fit model for run\n",
    "    model = get_model(max_features, embed_dim, embedding_matrix, lr, dropout, lstm_units)\n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev),epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[es,mc], shuffle=True)\n",
    "    \n",
    "    # load weights for best model\n",
    "    model.load_weights(\"best_model.h5\")\n",
    "\n",
    "    # save accuracy for best model\n",
    "    scores = model.evaluate(X_dev, y_dev, verbose=0)\n",
    "    #acc = scores[1]*100\n",
    "    loss = scores[0]\n",
    "    \n",
    "    # Store run results\n",
    "    #run_results = '(shuffle) ''accuracy: ' + str(acc) + \", \" + 'lr: ' + str(lr) + \", \" + 'batch: ' + str(batch_size) + \", \" + 'dropout: ' + str(dropout) + \", \" + 'lstm units: ' + str(lstm_units) + \"\\n\"\n",
    "    run_results = '(shuffle) ''val_loss: ' + str(loss) + \", \" + 'lr: ' + str(lr) + \", \" + 'batch: ' + str(batch_size) + \", \" + 'dropout: ' + str(dropout) + \", \" + 'lstm units: ' + str(lstm_units) + \"\\n\"\n",
    "    \n",
    "    f= open(\"lstm_run_results_val_loss.txt\",\"a+\")\n",
    "    f.write(run_results)\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
